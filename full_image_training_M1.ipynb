{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be59eb13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, top_k_accuracy_score, precision_recall_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torchcam.methods import GradCAM\n",
    "from torchvision import transforms\n",
    "from torchcam.utils import overlay_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce1236",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "First, the list of chosen bird species is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "195815a6",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "species = [\n",
    "    'Ciconia_ciconia', 'Columba_livia', 'Streptopelia_decaocto',\n",
    "    'Emberiza_calandra', 'Carduelis_carduelis', 'Serinus_serinus',\n",
    "    'Delichon_urbicum', 'Hirundo_rustica', 'Passer_domesticus',\n",
    "    'Sturnus_unicolor', 'Turdus_merula'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bb111",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "And some settings are defined for pre-processing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d2f68",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = 'saved_models/full_image_model'\n",
    "RESULT_DIR = 'images'  \n",
    "DATA_DIR = \"full_image_dataset\"\n",
    "DATASET = 'dataset_20250521.h5'\n",
    "BATCH_SIZE = [16]\n",
    "N_SPLITS = 5                            \n",
    "NUM_EPOCHS = 25\n",
    "NUM_CLASSES = 11\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20219c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openH5File(filepath, fold_idx=None):\n",
    "    file = h5py.File(filepath, 'r')\n",
    "    datasets = {}\n",
    "\n",
    "    if fold_idx is not None:\n",
    "        try:\n",
    "            fold_group = file[f'cross_validation/fold_{fold_idx}']\n",
    "            datasets['X_train'] = fold_group['X_train']\n",
    "            datasets['y_train'] = fold_group['y_train']\n",
    "            datasets['X_val'] = fold_group['X_val']\n",
    "            datasets['y_val'] = fold_group['y_val']\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Fold {fold_idx} not found in file. Available groups: {list(file['cross_validation'].keys())}\")\n",
    "    \n",
    "    datasets['X_test'] = file['test']['X_test']\n",
    "    datasets['y_test'] = file['test']['y_test']\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "def createDataloaders(X_h5, y_h5, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    X_np = X_h5[:]  # (N, H, W, C)\n",
    "    if X_np.ndim == 4:\n",
    "        X_np = np.transpose(X_np, (0, 3, 1, 2))  # to (N, C, H, W)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_np).float()\n",
    "    y_tensor = torch.from_numpy(y_h5[:]).long()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "def getDataloaders(filepath, fold_idx, batch_size):\n",
    "    dataset = openH5File(filepath, fold_idx)\n",
    "    test_loader = createDataloaders(dataset['X_test'], dataset['y_test'], batch_size=batch_size, shuffle=False)\n",
    "    if fold_idx is not None:\n",
    "        train_loader = createDataloaders(dataset['X_train'], dataset['y_train'], batch_size=batch_size, shuffle=True)\n",
    "        val_loader = createDataloaders(dataset['X_val'], dataset['y_val'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae37ea3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getModel(name, nClasses, dropout_rate=0):\n",
    "    if name == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier[1].in_features, nClasses)\n",
    "        )\n",
    "    elif name == 'efficientnet_V2':\n",
    "        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier[1].in_features, nClasses)\n",
    "        )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def getOptimizer(model, params):\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9, weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    return optimizer\n",
    "\n",
    "def trainModel(model, train_loader, val_loader, params):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getOptimizer(model, params)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3) if params['scheduler'] else None\n",
    "    \n",
    "    best_f1 = 0\n",
    "    THRESHOLD = 5\n",
    "    improvementCounter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': []}\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_running_corrects.double() / len(val_loader.dataset)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            improvementCounter = 0\n",
    "        else:\n",
    "            improvementCounter +=1\n",
    "            if improvementCounter >= THRESHOLD:\n",
    "                break\n",
    "\n",
    "    return history, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74aca8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(filepath, n_splits, hyperparams):\n",
    "    results_log = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"total_combinations\": len(list(itertools.product(*hyperparams.values()))),\n",
    "        \"best_f1\": 0,\n",
    "        \"best_params\": None,\n",
    "        \"all_results\": []\n",
    "    }\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    keys, values = zip(*hyperparams.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    print(f\"\\nBeginning GridSearch with {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for params in tqdm(param_combinations):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Testing combination: {params}\")\n",
    "        fold_f1_scores = []\n",
    "        fold_acc_scores = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation loop\n",
    "        for fold_idx in range(1, n_splits+1):\n",
    "            train_loader, val_loader, _ = getDataloaders(filepath, fold_idx, params['batch_size'])\n",
    "            model = getModel(params['model_name'], NUM_CLASSES, params.get('dropout_rate', 0))\n",
    "            model.to(DEVICE)\n",
    "            history, fold_f1 = trainModel(model, train_loader, val_loader, params)\n",
    "            print(f\"Fold {fold_idx} Best F1 Score: {fold_f1:.4f}\")\n",
    "            fold_f1_scores.append(fold_f1)\n",
    "            fold_acc_scores.append(history['val_acc'][-1].item())\n",
    "\n",
    "            # Clear memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate average F1 across folds\n",
    "        avg_f1 = np.mean(fold_f1_scores)\n",
    "        std_f1 = np.std(fold_f1_scores)\n",
    "        avg_acc = np.mean(fold_acc_scores)\n",
    "        std_acc = np.std(fold_acc_scores)\n",
    "        time_taken = time.time() - start_time\n",
    "\n",
    "        # Record this combination's results\n",
    "        result_entry = {\n",
    "            \"params\": params,\n",
    "            \"avg_f1\": avg_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_acc\": avg_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"f1_scores\": fold_f1_scores,\n",
    "            \"acc_scores\": fold_acc_scores,\n",
    "            \"memory_used_GB\": torch.cuda.max_memory_allocated()/1e9,\n",
    "            \"time_taken\": time_taken\n",
    "        }\n",
    "        results_log[\"all_results\"].append(result_entry)\n",
    "        \n",
    "        # Update best parameters if improved\n",
    "        if avg_f1 > results_log[\"best_f1\"]:\n",
    "            results_log[\"best_f1\"] = avg_f1\n",
    "            results_log[\"best_params\"] = params\n",
    "            print(f\"New best parameters found with F1: {results_log[\"best_f1\"]:.4f}\")\n",
    "\n",
    "    #Finalize results        \n",
    "    print(\"\\nGridSearch completed!\")\n",
    "    torch.save(results_log[\"best_params\"], os.path.join(MODEL_SAVE_DIR, f'gridsearch_setup1_{datetime.now().isoformat()}.pth'))\n",
    "\n",
    "    # Save JSON log\n",
    "    json_path = os.path.join(MODEL_SAVE_DIR, f\"gridsearch_results_{datetime.now().isoformat()}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results_log, f, indent=4)\n",
    "\n",
    "    # Save CSV results\n",
    "    csv_path = os.path.join(MODEL_SAVE_DIR, f\"gridsearch_results_{datetime.now().isoformat()}.csv\")\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"params\", \"avg_f1\", \"std_f1\", \"mean_acc\", \"std_acc\", \"f1_scores\", \"acc_scores\", \"memory_used_GB\", \"time_taken\"]\n",
    "        writer.writerow(header)\n",
    "        for res in results_log[\"all_results\"]:\n",
    "            writer.writerow([\n",
    "                str(res[\"params\"]), res[\"avg_f1\"], res[\"std_f1\"],\n",
    "                res[\"mean_acc\"], res[\"std_acc\"],\n",
    "                res[\"f1_scores\"], res[\"acc_scores\"],\n",
    "                res[\"memory_used_GB\"], res[\"time_taken\"]\n",
    "            ])\n",
    "\n",
    "def bestTrainModel(filepath, best_params):\n",
    "    model = getModel(best_params['model_name'], nClasses=NUM_CLASSES, dropout_rate=best_params['dropout_rate'])\n",
    "    train_loader, val_loader, test_loader = getDataloaders(filepath, fold_idx=1, batch_size=BATCH_SIZE[0])\n",
    "\n",
    "    history, _ = trainModel(model, train_loader, val_loader, best_params)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    all_probs = []\n",
    "    cam_images = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    top1_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_loss /= total_samples\n",
    "\n",
    "    labels_range = list(range(NUM_CLASSES))\n",
    "    try:\n",
    "        top3_acc = top_k_accuracy_score(all_labels, all_probs, k=3, labels=labels_range)\n",
    "    except ValueError:\n",
    "        top3_acc = 0.0  # fallback\n",
    "\n",
    "    try:\n",
    "        binarized_labels = label_binarize(all_labels, classes=labels_range)\n",
    "        auprc_macro = roc_auc_score(binarized_labels, all_probs, average='macro', multi_class='ovr')\n",
    "    except Exception:\n",
    "        auprc_macro = 0.0  # fallback\n",
    "\n",
    "    # Compute PR curve for the first class (just for plotting)\n",
    "    precision, recall, _ = precision_recall_curve(binarized_labels[:, 0], all_probs[:, 0])\n",
    "\n",
    "    metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top3_accuracy': top3_acc,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'macro_auprc': auprc_macro,\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist()\n",
    "    }\n",
    "\n",
    "    return model, history, metrics, cam_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fbcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(history, cm, metrics_dict, species, cam_images=None):\n",
    "    plt.figure(figsize=(24, 12))\n",
    "\n",
    "    # Plot training history\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(2, 3, 2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=species, yticklabels=species)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Plot precision-recall curve\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(metrics_dict['recall'],\n",
    "            metrics_dict['precision'], lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve (AUPRC: {metrics_dict[\"macro_auprc\"]:.4f})')\n",
    "\n",
    "    # Plot Grad-CAM visualizations\n",
    "    for i, (img, activation, label) in enumerate(cam_images[:3]):\n",
    "        plt.subplot(2, 3, 4+i)\n",
    "        result = overlay_mask(\n",
    "            to_pil_image(img), \n",
    "            to_pil_image(activation[0].squeeze(0), mode='F'), \n",
    "            alpha=0.5\n",
    "        )\n",
    "        plt.imshow(result)\n",
    "        plt.title(f'True: {species[label]}\\nPred: {species[torch.argmax(activation)]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(MODEL_SAVE_DIR, RESULT_DIR, f\"training_results_{datetime.now().isoformat()}.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save individual Grad-CAM images\n",
    "    for i, (img, activation, label) in enumerate(cam_images[:3]):\n",
    "        result = overlay_mask(\n",
    "            to_pil_image(img), \n",
    "            to_pil_image(activation[0].squeeze(0), mode='F'), \n",
    "            alpha=0.5\n",
    "        )\n",
    "        result.save(os.path.join(MODEL_SAVE_DIR, RESULT_DIR, f\"gradcam_{i}_{datetime.now().isoformat()}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b61ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning GridSearch with 16 combinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing combination: {'model_name': 'efficientnet_b0', 'learning_rate': 0.0005, 'batch_size': 16, 'weight_decay': 0.0001, 'optimizer': 'adamw', 'scheduler': True, 'dropout_rate': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'full_image_dataset/dataset_20250506.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(MODEL_SAVE_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_SAVE_DIR, RESULT_DIR), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m gridSearch(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, N_SPLITS, params)\n",
      "Cell \u001b[1;32mIn[14], line 25\u001b[0m, in \u001b[0;36mgridSearch\u001b[1;34m(filepath, n_splits, hyperparams)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Cross-validation loop\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_splits\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 25\u001b[0m     train_loader, val_loader, _ \u001b[38;5;241m=\u001b[39m getDataloaders(filepath, fold_idx, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m getModel(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m], NUM_CLASSES, params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m, in \u001b[0;36mgetDataloaders\u001b[1;34m(filepath, fold_idx, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetDataloaders\u001b[39m(filepath, fold_idx, batch_size):\n\u001b[1;32m---> 33\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m openH5File(filepath, fold_idx)\n\u001b[0;32m     34\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m createDataloaders(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m, in \u001b[0;36mopenH5File\u001b[1;34m(filepath, fold_idx)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopenH5File\u001b[39m(filepath, fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     file \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'full_image_dataset/dataset_20250506.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "# 1. Perform hyperparameter search\n",
    "params = {\n",
    "    'model_name': ['efficientnet_b0', 'efficientnet_V2'],\n",
    "    'learning_rate': [0.0005, 0.0001],\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'weight_decay': [0.0001, 0],\n",
    "    'optimizer': ['adamw'],\n",
    "    'scheduler': [True],\n",
    "    'dropout_rate': [0, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_SAVE_DIR, RESULT_DIR), exist_ok=True)\n",
    "\n",
    "gridSearch(f\"{DATA_DIR}/{DATASET}\", N_SPLITS, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS = 'gridsearch_setup1_2025-05-06T20:48:47.915540.pth'\n",
    "\n",
    "# 2. Train final model with best parameters\n",
    "best_params = torch.load(os.path.join(MODEL_SAVE_DIR, BEST_PARAMS))\n",
    "best_model, best_history, best_metrics, best_camImages  = bestTrainModel(f\"{DATA_DIR}/{DATASET}\", best_params)\n",
    "speciesModel = best_model.species if hasattr(best_model, 'species') else species\n",
    "\n",
    "# 3. Generate confusion matrix\n",
    "cm = best_metrics['confusion_matrix']\n",
    "    \n",
    "# 4. Plot results\n",
    "plotting(\n",
    "    history=best_history,\n",
    "    cm=cm,\n",
    "    metrics_dict=best_metrics,\n",
    "    species=speciesModel,\n",
    "    cam_images=best_camImages\n",
    ")\n",
    "    \n",
    "# 5. Save final model and metrics\n",
    "final_model_path = os.path.join(MODEL_SAVE_DIR, f'final_model_{datetime.now().isoformat()}.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': best_metrics,\n",
    "    'class_names': speciesModel,\n",
    "    'training_history': best_history\n",
    "}, final_model_path)\n",
    "\n",
    "# Save metrics separately\n",
    "with open(os.path.join(MODEL_SAVE_DIR, f\"final_metrics_{datetime.now().isoformat()}.json\"), 'w') as f:\n",
    "    json.dump({\n",
    "        'test_loss': best_metrics['test_loss'],\n",
    "        'top1_accuracy': best_metrics['top1_accuracy'],\n",
    "        'top3_accuracy': best_metrics['top3_accuracy'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'macro_auprc': best_metrics['macro_auprc'],\n",
    "        'precision_recall_curve': {\n",
    "            'precision': best_metrics['precision'],\n",
    "            'recall': best_metrics['recall']\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save confusion matrix\n",
    "np.save(os.path.join(MODEL_SAVE_DIR, f\"confusion_matrix_{datetime.now().isoformat()}.npy\"), cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
