{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a297c2e-7455-4ae1-835c-214c2198a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda8c1-f250-470b-8ea4-70471a77137b",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705814a6-1891-437b-8606-b825b45c9e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58acc433-0cf6-4f8d-a046-8ba1d38ab389",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0889c-f04f-47c6-b746-ab3d02999d1d",
   "metadata": {},
   "source": [
    "First, the list of chosen bird species is defined:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6254d96-640f-4265-8abf-702982a33cff",
   "metadata": {},
   "source": [
    "species = [\n",
    "    'Ciconia_ciconia', 'Columba_livia', 'Streptopelia_decaocto',\n",
    "    'Emberiza_calandra', 'Carduelis_carduelis', 'Serinus_serinus',\n",
    "    'Delichon_urbicum', 'Hirundo_rustica', 'Passer_domesticus',\n",
    "    'Sturnus_unicolor', 'Turdus_merula'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2ad40-3cec-4695-b0cf-b4e5cc437312",
   "metadata": {},
   "source": [
    "And some settings are defined for pre-processing the images."
   ]
  },
  {
   "cell_type": "raw",
   "id": "115671fb-7e7e-4ff7-a939-13a3a848f9c8",
   "metadata": {},
   "source": [
    "DATA_DIR = 'dataset_parts\\\\normal_dataset' \n",
    "OUTPUT_FILE = 'bird_dataset_segmented_pytorch.h5'  # Output HDF5 file\n",
    "IMG_SIZE = (224, 224)             # Standard size for CNNs\n",
    "TEST_SIZE = 0.1                   # Test set proportion\n",
    "COMPRESSION = 'gzip'              # Compression type\n",
    "COMPRESSION_LEVEL = 7             # Compression level (1-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ef7ad-f5c3-412a-967f-cea774210467",
   "metadata": {},
   "source": [
    "The images of the various birds must be transformed so that they can be used in the models, using PyTorch's transforms.Compose(). The transformations include data augmentation for the training set and basic preprocessing for the test set. But what is data augmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bf70c-7b21-45b8-8fd8-a4b90fee49f6",
   "metadata": {},
   "source": [
    "Data Augmentation is a technique used to expand a training dataset by creating modified versions of existing images through random but realistic transformations. It helps improve model generalization by exposing it to varied examples without collecting new data. Common transformations include flipping, rotating, scaling, changing brightness/contrast, adding noise, or cropping. These variations simulate different real-world scenarios, making the model more robust to changes in viewpoint, lighting, or orientation.\n",
    "\n",
    "Data augmentation is applied only during training—validation and test data remain unmodified to reflect real-world performance. It is especially useful for small datasets, reducing overfitting and improving accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145ecb5-dcb8-4b33-b2ff-86ad200dbf16",
   "metadata": {},
   "source": [
    "Let's now break down each component and explain the hyperparameters:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18388ee6-6ff1-43b4-b1c0-10bde163ef62",
   "metadata": {},
   "source": [
    "- transforms.Resize(IMG_SIZE)- Resizes the image to a fixed size. This size is typically chosen based on model architecture, in this case 224x224.\n",
    "\n",
    "- transforms.RandomHorizontalFlip()- Randomly flips the image horizontally with a default probability of 0.5.\n",
    "\n",
    "- transforms.RandomRotation(20)- Rotates the image randomly by up to ±20 degrees.\n",
    "\n",
    "- transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)- Randomly adjusts brightness, contrast, and saturation by up to ±10%.\n",
    "\n",
    "- transforms.ToTensor()- Converts the image to a PyTorch tensor (values scaled to [0, 1]).\n",
    "\n",
    "- transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])- Normalizes the image using precomputed mean and std from ImageNet.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cc29aa1-eaa6-4004-9bd3-b247dbc57094",
   "metadata": {},
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d89a7-56fc-4303-9e91-7245abe34d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
