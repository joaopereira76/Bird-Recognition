{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0e34d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import requests\n",
    "import h5py\n",
    "import psutil\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9265288",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"full_image_dataset\"          # Expected input dir: species_name/*.jpg\n",
    "AUGMENTED_DIR = \"augmented_dataset\"     # Augmented images will be saved here\n",
    "IMG_SIZE_THRESHOLD = 200              # Min resolution (px)\n",
    "HASH_THRESHOLD = 8                    # Duplicate threshold using phash\n",
    "\n",
    "species_keys = {\n",
    "    \"Carduelis carduelis\": 2494686,\n",
    "    \"Ciconia ciconia\": 2481912,\n",
    "    \"Columba livia\": 2495414,\n",
    "    \"Delichon urbicum\": 2489214,\n",
    "    \"Emberiza calandra\":7634625,\n",
    "    \"Hirundo rustica\": 7192162,\n",
    "    \"Passer domesticus\": 5231190,\n",
    "    \"Serinus serinus\":2494200,\n",
    "    \"Streptopelia decaocto\": 2495696,\n",
    "    \"Sturnus unicolor\":2489104,\n",
    "    \"Turdus merula\": 6171845   \n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.15,\n",
    "    'N_SPLITS': 5,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 6,\n",
    "    'SAVE_AS_JPEG': True,\n",
    "    'JPEG_QUALITY': 80,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomResizedCrop','size':(224,224) , 'scale': (0.8, 1.0)},\n",
    "            {'name': 'HorizontalFlip', 'p': 0.5},\n",
    "            {'name': 'ShiftScaleRotate', 'shift_limit': 0.05, 'scale_limit': 0.1, 'rotate_limit': 20, 'p': 0.7},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'hue': 0.05, 'p': 0.8},\n",
    "            {'name': 'CoarseDropout', 'max_holes':1, 'max_height': 48, 'max_width': 48, 'p': 0.4},\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b52404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"os\": platform.system(),\n",
    "        \"os_version\": platform.release(),\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"cpu_cores\": psutil.cpu_count(logical=False),\n",
    "        \"ram_total_gb\": round(mem.total / (1024**3), 2),\n",
    "        \"ram_available_gb\": round(mem.available / (1024**3), 2),\n",
    "        \"python_version\": platform.python_version()\n",
    "    }\n",
    "\n",
    "def initLogging(output_dir):\n",
    "    metadata = {\n",
    "        \"config\": CONFIG,\n",
    "        \"system\": getSystemInfo(),\n",
    "        \"download\": {},\n",
    "        \"cleaning\": {},\n",
    "        \"augmentation\": {},\n",
    "        \"dataset_stats\": {},\n",
    "        \"processing_times\": {}\n",
    "    }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, f\"dataset_prep_{datetime.now().strftime(\"%Y%m%d\")}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata_path\n",
    "\n",
    "def updateLogging(metadata_path, updates):\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return initLogging(os.path.dirname(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    metadata.update(updates)\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd2301b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAugmentation():\n",
    "    aug_config = CONFIG['AUGMENTATION']['train']\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(\n",
    "            size=aug_config[0]['size'],\n",
    "            scale=aug_config[0]['scale'],\n",
    "        ),\n",
    "        A.HorizontalFlip(p=aug_config[1]['p']),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=aug_config[2]['shift_limit'],\n",
    "            scale_limit=aug_config[2]['scale_limit'],\n",
    "            rotate_limit=aug_config[2]['rotate_limit'],\n",
    "            p=aug_config[2]['p']\n",
    "        ),\n",
    "        A.ColorJitter(\n",
    "            brightness=aug_config[3]['brightness'],\n",
    "            contrast=aug_config[3]['contrast'],\n",
    "            saturation=aug_config[3]['saturation'],\n",
    "            hue=aug_config[3]['hue'],\n",
    "            p=aug_config[3]['p']\n",
    "        ),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=aug_config[4]['max_holes'],\n",
    "            max_height=aug_config[4]['max_height'],\n",
    "            max_width=aug_config[4]['max_width'],\n",
    "            p=aug_config[4]['p']\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def processImage(img_path, output_dir, transform, save_augmented=True):\n",
    "    \"\"\"Process and save a single image with augmentation\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=img_np)['image']\n",
    "        \n",
    "        if save_augmented:\n",
    "            # Save augmented image\n",
    "            aug_name = f\"{Path(img_path).stem}_aug.jpg\"\n",
    "            aug_path = os.path.join(output_dir, aug_name)\n",
    "            Image.fromarray(augmented).save(aug_path, quality=CONFIG['JPEG_QUALITY'], optimize=True)\n",
    "            return True\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def transformImagesFromDirectory(species_name, data_dir, metadata_path=None, save_augmented=True):\n",
    "    start_time = time.time()\n",
    "    species_dir = os.path.join(data_dir, species_name.replace(\" \", \"_\"))\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(data_dir)\n",
    "\n",
    "    print(f\"\\nApplying transformations to images for: {species_name}\")\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'original_count': 0,\n",
    "        'augmented_saved': 0,\n",
    "        'start_time': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    }\n",
    "\n",
    "    # Create output directory\n",
    "    if save_augmented:\n",
    "        output_dir = os.path.join(data_dir, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = species_dir\n",
    "\n",
    "    # Get augmentation pipeline\n",
    "    transform = getAugmentation()\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = [os.path.join(species_dir, f) for f in os.listdir(species_dir) \n",
    "                  if os.path.isfile(os.path.join(species_dir, f)) and not f.endswith(\".json\")]\n",
    "    \n",
    "    stats['original_count'] = len(image_paths)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: processImage(p, output_dir, transform, save_augmented),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Augmenting {species_name}\"\n",
    "        ))\n",
    "    \n",
    "    stats['augmented_saved'] = sum(results)\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().isoformat(),\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "\n",
    "    updateLogging(metadata_path, {\"augmentation\": {species_name: stats}})\n",
    "    print(f\"Transformations completed for {species_name}. Augmented saved: {stats['augmented_saved']}\")\n",
    "    return stats['augmented_saved']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fc53484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return min(img.size) >= IMG_SIZE_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def getPhash(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return imagehash.phash(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanData(species_name, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    hash_db = defaultdict(list)\n",
    "    total_removed = 0\n",
    "    total_valid_images = 0\n",
    "\n",
    "    species_path = os.path.join(DATA_DIR, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "    if not os.path.exists(species_path):\n",
    "        return 0\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = list(Path(species_path).glob(\"*.*\"))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: (p, isValidImage(p), getPhash(p)),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Cleaning {species_name}\"\n",
    "        ))\n",
    "\n",
    "    # Process results\n",
    "    for img_path, is_valid, phash in results:\n",
    "        if not is_valid or phash is None:\n",
    "            os.remove(img_path)\n",
    "            total_removed += 1\n",
    "        elif not any(phash - existing < HASH_THRESHOLD for existing in hash_db[species_name]):\n",
    "            hash_db[species_name].append(phash)\n",
    "            total_valid_images += 1\n",
    "        else:\n",
    "            os.remove(img_path)\n",
    "            total_removed += 1\n",
    "\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'total_removed': total_removed,\n",
    "        'remaining_images': total_valid_images,\n",
    "        'time_seconds': time.time() - start_time,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    updateLogging(metadata_path, {\"cleaning\": {species_name: stats}})\n",
    "    print(f\"Finished cleaning {species_name}. Removed {total_removed} images\")\n",
    "    return total_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "810f3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    # Collect all images and labels\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    species_counts = defaultdict(int)\n",
    "    \n",
    "    for species_idx, (species_name, _) in enumerate(species_keys.items()):\n",
    "        species_dir = os.path.join(DATA_DIR, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        if not os.path.exists(species_dir):\n",
    "            continue\n",
    "            \n",
    "        for img_name in os.listdir(species_dir):\n",
    "            img_path = os.path.join(species_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                all_images.append(np.array(img))\n",
    "                all_labels.append(species_idx)\n",
    "                species_counts[species_name] += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_images)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    # Create splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['TEST_SIZE'], stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Save CSV metadata\n",
    "    csv_path = os.path.join(DATA_DIR, f\"dataset_metadata_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "    df = pd.DataFrame({\n",
    "        'species': [list(species_keys.keys())[i] for i in y],\n",
    "        'label_idx': y,\n",
    "        'filepath': [f\"{list(species_keys.keys())[i]}/{j}.jpg\" \n",
    "                    for i, j in zip(y, range(len(y)))]\n",
    "    })\n",
    "\n",
    "    # Add fold information\n",
    "    df['fold'] = -1\n",
    "    skf = StratifiedGroupKFold(\n",
    "        n_splits=CONFIG['N_SPLITS']\n",
    "    )\n",
    "    for fold_idx, (_, val_idx) in enumerate(skf.split(df, df['label_idx'])):\n",
    "        df.loc[val_idx, 'fold'] = fold_idx\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save HDF5 dataset in chunks\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "    h5_path = os.path.join(DATA_DIR, f\"dataset_{timestamp}.h5\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'w') as hf:\n",
    "        # Save test set\n",
    "        test_group = hf.create_group('test')\n",
    "        test_group.create_dataset('X_test', data=X_test,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        test_group.create_dataset('y_test', data=y_test,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Save cross-validation folds\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            fold_group = cv_group.create_group(f'fold_{fold+1}')\n",
    "            fold_group.create_dataset('X_train', data=X_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Save metadata\n",
    "        hf.attrs['species'] = json.dumps(list(species_keys.keys()))\n",
    "        hf.attrs['image_size'] = json.dumps(CONFIG['IMG_SIZE'])\n",
    "        hf.attrs['augmentation'] = json.dumps(CONFIG['AUGMENTATION'])\n",
    "        hf.attrs['creation_time'] = timestamp\n",
    "        hf.attrs['csv_reference'] = csv_path\n",
    "\n",
    "    # Update metadata\n",
    "    dataset_stats = {\n",
    "        'total_images': len(all_images),\n",
    "        'species_counts': dict(species_counts),\n",
    "        'h5_path': h5_path,\n",
    "        'csv_path': csv_path,\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'processing_time_seconds': time.time() - start_time,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    updateLogging(metadata_path, {\"dataset_stats\": dataset_stats})\n",
    "    print(f\"Dataset created with {len(all_images)} images\")\n",
    "    print(f\"- HDF5: {h5_path}\")\n",
    "    print(f\"- CSV: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "007f3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w4ter/anaconda3/envs/AP/lib/python3.13/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_224423/2518849525.py:22: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset transformation...\n",
      "\n",
      "Applying transformations to images for: Carduelis carduelis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Carduelis carduelis: 100%|██████████| 600/600 [00:04<00:00, 125.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Carduelis carduelis. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Carduelis carduelis: 100%|██████████| 600/600 [00:02<00:00, 296.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Carduelis carduelis. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Ciconia ciconia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Ciconia ciconia: 100%|██████████| 600/600 [00:05<00:00, 110.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Ciconia ciconia. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Ciconia ciconia: 100%|██████████| 600/600 [00:01<00:00, 301.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Ciconia ciconia. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Columba livia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Columba livia: 100%|██████████| 600/600 [00:10<00:00, 57.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Columba livia. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Columba livia: 100%|██████████| 600/600 [00:01<00:00, 357.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Columba livia. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Delichon urbicum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Delichon urbicum: 100%|██████████| 600/600 [00:07<00:00, 79.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Delichon urbicum. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Delichon urbicum: 100%|██████████| 600/600 [00:02<00:00, 291.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Delichon urbicum. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Emberiza calandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Emberiza calandra: 100%|██████████| 600/600 [00:03<00:00, 154.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Emberiza calandra. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Emberiza calandra: 100%|██████████| 600/600 [00:01<00:00, 343.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Emberiza calandra. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Hirundo rustica\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Hirundo rustica: 100%|██████████| 600/600 [00:05<00:00, 115.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Hirundo rustica. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Hirundo rustica: 100%|██████████| 600/600 [00:02<00:00, 293.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Hirundo rustica. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Passer domesticus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Passer domesticus: 100%|██████████| 600/600 [00:05<00:00, 107.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Passer domesticus. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Passer domesticus: 100%|██████████| 600/600 [00:01<00:00, 306.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Passer domesticus. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Serinus serinus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Serinus serinus: 100%|██████████| 600/600 [00:10<00:00, 55.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Serinus serinus. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Serinus serinus: 100%|██████████| 600/600 [00:01<00:00, 312.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Serinus serinus. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Streptopelia decaocto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Streptopelia decaocto: 100%|██████████| 600/600 [00:10<00:00, 56.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Streptopelia decaocto. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Streptopelia decaocto: 100%|██████████| 600/600 [00:01<00:00, 343.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Streptopelia decaocto. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Sturnus unicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Sturnus unicolor: 100%|██████████| 600/600 [00:04<00:00, 142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Sturnus unicolor. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Sturnus unicolor: 100%|██████████| 600/600 [00:01<00:00, 307.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Sturnus unicolor. Removed 0 images\n",
      "\n",
      "Applying transformations to images for: Turdus merula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Turdus merula: 100%|██████████| 600/600 [00:07<00:00, 79.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations completed for Turdus merula. Augmented saved: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Turdus merula: 100%|██████████| 600/600 [00:01<00:00, 360.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Turdus merula. Removed 2 images\n",
      "Dataset created with 6598 images\n",
      "- HDF5: full_image_dataset/dataset_20250519.h5\n",
      "- CSV: full_image_dataset/dataset_metadata_20250519.csv\n",
      "Tasks completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating dataset transformation...\")\n",
    "metadata_path = initLogging(DATA_DIR)\n",
    "for species in species_keys.keys():\n",
    "    transformImagesFromDirectory(species, DATA_DIR, metadata_path)\n",
    "    cleanData(species, metadata_path=metadata_path)\n",
    "createDataset(metadata_path)\n",
    "print(\"Tasks completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
