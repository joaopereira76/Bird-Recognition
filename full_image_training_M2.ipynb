{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be59eb13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, top_k_accuracy_score, precision_recall_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torchcam.methods import GradCAM\n",
    "from torchvision import transforms\n",
    "from torchcam.utils import overlay_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce1236",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "First, the list of chosen bird species is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195815a6",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "species = [\n",
    "    'Ciconia_ciconia', 'Columba_livia', 'Streptopelia_decaocto',\n",
    "    'Emberiza_calandra', 'Carduelis_carduelis', 'Serinus_serinus',\n",
    "    'Delichon_urbicum', 'Hirundo_rustica', 'Passer_domesticus',\n",
    "    'Sturnus_unicolor', 'Turdus_merula'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bb111",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "And some settings are defined for pre-processing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8d2f68",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = 'saved_models/full_image_model'\n",
    "RESULT_DIR = 'images'  \n",
    "DATA_DIR = \"full_image_dataset\"\n",
    "AUGMENTED_DATA_DIR = \"augmented_dataset\"\n",
    "DATASET = 'dataset_20250519.h5'\n",
    "BATCH_SIZE = [16]\n",
    "N_SPLITS = 5                            \n",
    "NUM_EPOCHS = 25\n",
    "NUM_CLASSES = len(species)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Experiment B parameters\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "USE_MIXUP = True\n",
    "USE_CUTMIX = True\n",
    "\n",
    "# Experiment E parameters\n",
    "UNCERTAINTY_THRESHOLD = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20219c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openH5File(filepath, fold_idx=None):\n",
    "    file = h5py.File(filepath, 'r')\n",
    "    datasets = {}\n",
    "\n",
    "    if fold_idx is not None:\n",
    "        try:\n",
    "            fold_group = file[f'cross_validation/fold_{fold_idx}']\n",
    "            datasets['X_train'] = fold_group['X_train']\n",
    "            datasets['y_train'] = fold_group['y_train']\n",
    "            datasets['X_val'] = fold_group['X_val']\n",
    "            datasets['y_val'] = fold_group['y_val']\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Fold {fold_idx} not found in file. Available groups: {list(file['cross_validation'].keys())}\")\n",
    "    \n",
    "    datasets['X_test'] = file['test']['X_test']\n",
    "    datasets['y_test'] = file['test']['y_test']\n",
    "    return datasets\n",
    "\n",
    "def createDataloaders(X_h5, y_h5, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    X_np = X_h5[:]  # (N, H, W, C)\n",
    "    if X_np.ndim == 4:\n",
    "        X_np = np.transpose(X_np, (0, 3, 1, 2))  # to (N, C, H, W)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_np).float()\n",
    "    y_tensor = torch.from_numpy(y_h5[:]).long()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, num_workers=4, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "def getDataloaders(filepath, fold_idx, batch_size):\n",
    "    dataset = openH5File(f\"{filepath}\", fold_idx)\n",
    "    test_loader = createDataloaders(dataset['X_test'], dataset['y_test'], batch_size)\n",
    "    if fold_idx is not None:\n",
    "        train_loader = createDataloaders(dataset['X_train'], dataset['y_train'], batch_size, shuffle=True)\n",
    "        val_loader = createDataloaders(dataset['X_val'], dataset['y_val'], batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae37ea3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getModel(name, nClasses, dropout_rate=0):\n",
    "    if name == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier[1].in_features, nClasses)\n",
    "        )\n",
    "    elif name == 'efficientnet_V2':\n",
    "        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(model.classifier[1].in_features, nClasses)\n",
    "        )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def getOptimizer(model, params):\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9, weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def apply_mixup_cutmix(inputs, labels, params):\n",
    "    \"\"\"Applies MixUp or CutMix augmentation to a batch\"\"\"\n",
    "    if not (USE_MIXUP or USE_CUTMIX):\n",
    "        return inputs, (labels, labels, 1.0)\n",
    "    \n",
    "    use_mixup = params.get('use_mixup', False)\n",
    "    use_cutmix = params.get('use_cutmix', False)\n",
    "    \n",
    "    if use_cutmix:\n",
    "        lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n",
    "        b, _, h, w = inputs.size()\n",
    "        \n",
    "        # Generate random bounding box\n",
    "        rx = random.randint(0, w)\n",
    "        ry = random.randint(0, h)\n",
    "        rw = int(w * np.sqrt(1 - lam))\n",
    "        rh = int(h * np.sqrt(1 - lam))\n",
    "        x1 = max(0, rx - rw // 2)\n",
    "        y1 = max(0, ry - rh // 2)\n",
    "        x2 = min(w, x1 + rw)\n",
    "        y2 = min(h, y1 + rh)\n",
    "        \n",
    "        # Apply CutMix\n",
    "        inputs[:, :, y1:y2, x1:x2] = inputs.flip(0)[:, :, y1:y2, x1:x2]\n",
    "        lam = 1 - (x2 - x1) * (y2 - y1) / (w * h)\n",
    "    else:\n",
    "        # Apply MixUp\n",
    "        lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
    "        inputs = lam * inputs + (1 - lam) * inputs.flip(0)\n",
    "    \n",
    "    # Return mixed labels and lambda\n",
    "    return inputs, (labels, labels.flip(0), lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1127e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, train_loader, val_loader, params):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getOptimizer(model, params)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3) if params['scheduler'] else None\n",
    "    \n",
    "    best_f1 = 0\n",
    "    THRESHOLD = 5\n",
    "    improvementCounter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': []}\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Apply MixUp/CutMix only during training\n",
    "            if USE_MIXUP or USE_CUTMIX:\n",
    "                inputs, (labels1, labels2, lam) = apply_mixup_cutmix(inputs, labels, params)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if USE_MIXUP or USE_CUTMIX:\n",
    "                loss = lam * criterion(outputs, labels1) + (1 - lam) * criterion(outputs, labels2)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_running_corrects.double() / len(val_loader.dataset)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            improvementCounter = 0\n",
    "        else:\n",
    "            improvementCounter += 1\n",
    "            if improvementCounter >= THRESHOLD:\n",
    "                break\n",
    "\n",
    "    return history, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74aca8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(filepath, n_splits, hyperparams):\n",
    "    results_log = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        \"total_combinations\": len(list(itertools.product(*hyperparams.values()))),\n",
    "        \"best_f1\": 0,\n",
    "        \"best_params\": None,\n",
    "        \"all_results\": []\n",
    "    }\n",
    "\n",
    "    # Generate all possible hyperparameter combinations\n",
    "    keys, values = zip(*hyperparams.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    print(f\"\\nBeginning GridSearch with {len(param_combinations)} combinations...\")\n",
    "    \n",
    "    for params in tqdm(param_combinations):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Testing combination: {params}\")\n",
    "        fold_f1_scores = []\n",
    "        fold_acc_scores = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation loop\n",
    "        for fold_idx in range(1, n_splits+1):\n",
    "            train_loader, val_loader, _ = getDataloaders(filepath, fold_idx, params['batch_size'])\n",
    "            model = getModel(params['model_name'], NUM_CLASSES, params.get('dropout_rate', 0))\n",
    "            model.to(DEVICE)\n",
    "            history, fold_f1 = trainModel(model, train_loader, val_loader, params)\n",
    "            print(f\"Fold {fold_idx} Best F1 Score: {fold_f1:.4f}\")\n",
    "            fold_f1_scores.append(fold_f1)\n",
    "            fold_acc_scores.append(history['val_acc'][-1].item())\n",
    "\n",
    "            # Clear memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate average F1 across folds\n",
    "        avg_f1 = np.mean(fold_f1_scores)\n",
    "        std_f1 = np.std(fold_f1_scores)\n",
    "        avg_acc = np.mean(fold_acc_scores)\n",
    "        std_acc = np.std(fold_acc_scores)\n",
    "        time_taken = time.time() - start_time\n",
    "\n",
    "        # Record this combination's results\n",
    "        result_entry = {\n",
    "            \"params\": params,\n",
    "            \"avg_f1\": avg_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_acc\": avg_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"f1_scores\": fold_f1_scores,\n",
    "            \"acc_scores\": fold_acc_scores,\n",
    "            \"memory_used_GB\": torch.cuda.max_memory_allocated()/1e9,\n",
    "            \"time_taken\": time_taken\n",
    "        }\n",
    "        results_log[\"all_results\"].append(result_entry)\n",
    "        \n",
    "        # Update best parameters if improved\n",
    "        if avg_f1 > results_log[\"best_f1\"]:\n",
    "            results_log[\"best_f1\"] = avg_f1\n",
    "            results_log[\"best_params\"] = params\n",
    "            print(f\"New best parameters found with F1: {avg_f1:.4f}\")\n",
    "\n",
    "    # Finalize results        \n",
    "    print(\"\\nGridSearch completed!\")\n",
    "    torch.save(results_log[\"best_params\"], os.path.join(MODEL_SAVE_DIR, f'gridsearch_setup1_{datetime.now().strftime(\"%Y%m%d\")}.pth'))\n",
    "\n",
    "    # Save JSON log\n",
    "    json_path = os.path.join(MODEL_SAVE_DIR, f\"gridsearch_results_{datetime.now().strftime(\"%Y%m%d\")}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results_log, f, indent=4)\n",
    "\n",
    "    # Save CSV results\n",
    "    csv_path = os.path.join(MODEL_SAVE_DIR, f\"gridsearch_results_{datetime.now().strftime(\"%Y%m%d\")}.csv\")\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"params\", \"avg_f1\", \"std_f1\", \"mean_acc\", \"std_acc\", \"f1_scores\", \"acc_scores\", \"memory_used_GB\", \"time_taken\"]\n",
    "        writer.writerow(header)\n",
    "        for res in results_log[\"all_results\"]:\n",
    "            writer.writerow([\n",
    "                str(res[\"params\"]), res[\"avg_f1\"], res[\"std_f1\"],\n",
    "                res[\"mean_acc\"], res[\"std_acc\"],\n",
    "                res[\"f1_scores\"], res[\"acc_scores\"],\n",
    "                res[\"memory_used_GB\"], res[\"time_taken\"]\n",
    "            ])\n",
    "\n",
    "def bestTrainModel(filepath, best_params):\n",
    "    model = getModel(best_params['model_name'], nClasses=NUM_CLASSES, dropout_rate=best_params['dropout_rate'])\n",
    "    train_loader, val_loader, test_loader = getDataloaders(filepath, fold_idx=1, batch_size=BATCH_SIZE[0])\n",
    "\n",
    "    history, _ = trainModel(model, train_loader, val_loader, best_params)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    all_probs = []\n",
    "    cam_images = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            max_probs, preds = torch.max(probs, dim=1)\n",
    "            \n",
    "            # Apply uncertainty threshold (Experiment E)\n",
    "            uncertain_mask = max_probs < UNCERTAINTY_THRESHOLD\n",
    "            preds[uncertain_mask] = -1  # Mark as uncertain\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Filter out uncertain predictions for metrics calculation\n",
    "    certain_mask = np.array(all_preds) != -1\n",
    "    certain_preds = np.array(all_preds)[certain_mask]\n",
    "    certain_labels = np.array(all_labels)[certain_mask]\n",
    "    certain_probs = np.array(all_probs)[certain_mask]\n",
    "\n",
    "    cm = confusion_matrix(certain_labels, certain_preds)\n",
    "    f1 = f1_score(certain_labels, certain_preds, average='weighted')\n",
    "    top1_acc = accuracy_score(certain_labels, certain_preds)\n",
    "    test_loss /= total_samples\n",
    "\n",
    "    labels_range = list(range(NUM_CLASSES))\n",
    "    try:\n",
    "        top3_acc = top_k_accuracy_score(certain_labels, certain_probs, k=3, labels=labels_range)\n",
    "    except ValueError:\n",
    "        top3_acc = 0.0\n",
    "\n",
    "    try:\n",
    "        binarized_labels = label_binarize(certain_labels, classes=labels_range)\n",
    "        auprc_macro = roc_auc_score(binarized_labels, certain_probs, average='macro', multi_class='ovr')\n",
    "    except Exception:\n",
    "        auprc_macro = 0.0\n",
    "\n",
    "    # Compute PR curve for the first class (just for plotting)\n",
    "    precision, recall, _ = precision_recall_curve(binarized_labels[:, 0], certain_probs[:, 0])\n",
    "\n",
    "    metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top3_accuracy': top3_acc,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'macro_auprc': auprc_macro,\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'uncertainty_rate': 1 - (certain_mask.sum() / len(all_preds)),\n",
    "        'threshold': UNCERTAINTY_THRESHOLD\n",
    "    }\n",
    "\n",
    "    return model, history, metrics, cam_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fbcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(history, cm, metrics_dict, species, cam_images=None):\n",
    "    plt.figure(figsize=(24, 12))\n",
    "\n",
    "    # Plot training history\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.subplot(2, 3, 2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=species, yticklabels=species)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Plot precision-recall curve\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(metrics_dict['recall'],\n",
    "            metrics_dict['precision'], lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve (AUPRC: {metrics_dict[\"macro_auprc\"]:.4f})')\n",
    "\n",
    "    # Plot Grad-CAM visualizations\n",
    "    if cam_images:\n",
    "        for i, (img, activation, label) in enumerate(cam_images[:3]):\n",
    "            plt.subplot(2, 3, 4+i)\n",
    "            result = overlay_mask(\n",
    "                to_pil_image(img), \n",
    "                to_pil_image(activation[0].squeeze(0), mode='F'), \n",
    "                alpha=0.5\n",
    "            )\n",
    "            plt.imshow(result)\n",
    "            plt.title(f'True: {species[label]}\\nPred: {species[torch.argmax(activation)]}')\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(MODEL_SAVE_DIR, RESULT_DIR, f\"training_results_{datetime.now().strftime(\"%Y%m%d\")}.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save individual Grad-CAM images\n",
    "    if cam_images:\n",
    "        for i, (img, activation, label) in enumerate(cam_images[:3]):\n",
    "            result = overlay_mask(\n",
    "                to_pil_image(img), \n",
    "                to_pil_image(activation[0].squeeze(0), mode='F'), \n",
    "                alpha=0.5\n",
    "            )\n",
    "            result.save(os.path.join(MODEL_SAVE_DIR, RESULT_DIR, f\"gradcam_{i}_{datetime.now().strftime(\"%Y%m%d\")}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1baa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(MODEL_SAVE_DIR, RESULT_DIR), exist_ok=True)\n",
    "BEST_PARAMS = 'gridsearch_setup1_20250506.pth'\n",
    "best_params = torch.load(os.path.join(MODEL_SAVE_DIR, BEST_PARAMS))\n",
    "best_model, best_history, best_metrics, best_camImages = bestTrainModel(f\"{DATA_DIR}/{DATASET}\", best_params)\n",
    "speciesModel = best_model.species if hasattr(best_model, 'species') else species\n",
    "\n",
    "#Generate confusion matrix\n",
    "cm = best_metrics['confusion_matrix']\n",
    "        \n",
    "#Plot results\n",
    "plotting(\n",
    "    history=best_history,\n",
    "    cm=cm,\n",
    "    metrics_dict=best_metrics,\n",
    "    species=speciesModel,\n",
    "    cam_images=best_camImages\n",
    ")\n",
    "        \n",
    "# Save final model and metrics\n",
    "final_model_path = os.path.join(MODEL_SAVE_DIR, f'final_model_{datetime.now().strftime(\"%Y%m%d\")}.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': best_metrics,\n",
    "    'class_names': speciesModel,\n",
    "    'training_history': best_history\n",
    "}, final_model_path)\n",
    "\n",
    "# Save metrics separately\n",
    "with open(os.path.join(MODEL_SAVE_DIR, f\"final_metrics_{datetime.now().strftime(\"%Y%m%d\")}.json\"), 'w') as f:\n",
    "    json.dump({\n",
    "        'test_loss': best_metrics['test_loss'],\n",
    "        'top1_accuracy': best_metrics['top1_accuracy'],\n",
    "        'top3_accuracy': best_metrics['top3_accuracy'],\n",
    "        'f1_score': best_metrics['f1_score'],\n",
    "        'macro_auprc': best_metrics['macro_auprc'],\n",
    "        'precision_recall_curve': {\n",
    "            'precision': best_metrics['precision'],\n",
    "            'recall': best_metrics['recall']\n",
    "        },\n",
    "        'uncertainty_rate': best_metrics['uncertainty_rate'],\n",
    "        'threshold': best_metrics['threshold']\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save confusion matrix\n",
    "np.save(os.path.join(MODEL_SAVE_DIR, f\"confusion_matrix_{datetime.now().strftime(\"%Y%m%d\")}.npy\"), cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
