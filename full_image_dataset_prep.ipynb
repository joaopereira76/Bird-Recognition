{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aa331ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import requests\n",
    "import h5py\n",
    "import psutil\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, StratifiedKFold\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "\n",
    "from pygbif import occurrences\n",
    "from pyinaturalist.node_api import get_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd6ff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"full_image_dataset\"          # Expected input dir: species_name/*.jpg\n",
    "AUGMENTED_DIR = \"augmented_dataset\"     # Augmented images will be saved here\n",
    "IMG_SIZE_THRESHOLD = 200              # Min resolution (px)\n",
    "HASH_THRESHOLD = 8                    # Duplicate threshold using phash\n",
    "\n",
    "species_keys = {\n",
    "    \"Carduelis carduelis\": 2494686,\n",
    "    \"Ciconia ciconia\": 2481912,\n",
    "    \"Columba livia\": 2495414,\n",
    "    \"Delichon urbicum\": 2489214,\n",
    "    \"Emberiza calandra\":7634625,\n",
    "    \"Hirundo rustica\": 7192162,\n",
    "    \"Passer domesticus\": 5231190,\n",
    "    \"Serinus serinus\":2494200,\n",
    "    \"Streptopelia decaocto\": 2495696,\n",
    "    \"Sturnus unicolor\":2489104,\n",
    "    \"Turdus merula\": 6171845   \n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.15,\n",
    "    'N_SPLITS': 5,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 6,\n",
    "    'SAVE_AS_JPEG': True,\n",
    "    'JPEG_QUALITY': 80,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomResizedCrop','size':(224,224) , 'scale': (0.8, 1.0)},\n",
    "            {'name': 'HorizontalFlip', 'p': 0.5},\n",
    "            {'name': 'ShiftScaleRotate', 'shift_limit': 0.05, 'scale_limit': 0.1, 'rotate_limit': 20, 'p': 0.7},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'hue': 0.05, 'p': 0.8},\n",
    "            {'name': 'CoarseDropout', 'max_holes':1, 'max_height': 48, 'max_width': 48, 'p': 0.4},\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4afd675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"os\": platform.system(),\n",
    "        \"os_version\": platform.release(),\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"cpu_cores\": psutil.cpu_count(logical=False),\n",
    "        \"ram_total_gb\": round(mem.total / (1024**3), 2),\n",
    "        \"ram_available_gb\": round(mem.available / (1024**3), 2),\n",
    "        \"python_version\": platform.python_version()\n",
    "    }\n",
    "\n",
    "def initLogging(output_dir):\n",
    "    metadata = {\n",
    "        \"config\": CONFIG,\n",
    "        \"system\": getSystemInfo(),\n",
    "        \"download\": {},\n",
    "        \"cleaning\": {},\n",
    "        \"dataset_stats\": {},\n",
    "    }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, f\"dataset_prep_{datetime.now().strftime(\"%Y%m%d\")}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata_path\n",
    "\n",
    "def updateLogging(metadata_path, updates):\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return initLogging(os.path.dirname(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    metadata.update(updates)\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52038183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadImages(species_name, output_dir, limit=500, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    output_dir = os.path.join(DATA_DIR, species_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    print(f\"\\nDownloading images for: {species_name}\")\n",
    "    stats = {\n",
    "        'iNaturalist': 0,\n",
    "        'GBIF': 0,\n",
    "        'start_time': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #iNaturalist download\n",
    "        stats['iNaturalist'] = downloadImages_INaturalist(species_name, output_dir, limit)\n",
    "        \n",
    "        # GBIF download\n",
    "        current_count = stats['iNaturalist']\n",
    "        stats['GBIF'] = downloadImages_GBIF(species_name, current_count, output_dir, limit - current_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "    \n",
    "    # Update metadata\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        'total_downloaded': stats['iNaturalist'] + stats['GBIF'],\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    updateLogging(metadata_path, {\"download\": {species_name: stats}})\n",
    "    print(f\"Total images downloaded for {species_name}: {stats['total_downloaded']}\")\n",
    "    return stats['total_downloaded']\n",
    "\n",
    "def downloadImages_INaturalist(species_name, output_dir, limit=500):\n",
    "    results = get_observations(\n",
    "        taxon_name=species_name,\n",
    "        per_page=limit,\n",
    "        quality_grade=\"research\",\n",
    "        media_type=\"photo\",\n",
    "        license=[\"CC-BY\",\"CC-BY-NC\"] \n",
    "    )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for photo in obs.get(\"photos\",[]):\n",
    "            url = photo.get(\"url\", \"\").replace(\"square\", \"original\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{images_downloaded}.{image_ext}\"\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Downloaded {images_downloaded} images from iNaturalist for {species_name}\")\n",
    "    return images_downloaded\n",
    "\n",
    "def downloadImages_GBIF(species_name, downloadedValue, output_dir, limit=500):\n",
    "    results = occurrences.search(\n",
    "            taxonKey=species_keys[species_name],\n",
    "            mediaType=\"StillImage\",\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for media in obs.get(\"media\",[]):\n",
    "            url = media.get(\"identifier\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{downloadedValue + images_downloaded}.{image_ext}\"\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break   \n",
    "    print(f\"\\nDownloaded {images_downloaded} images from GBIF for {species_name}\")\n",
    "    return images_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12d3d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAugmentation():\n",
    "    aug_config = CONFIG['AUGMENTATION']['train']\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(\n",
    "            size=aug_config[0]['size'],\n",
    "            scale=aug_config[0]['scale'],\n",
    "        ),\n",
    "        A.HorizontalFlip(p=aug_config[1]['p']),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=aug_config[2]['shift_limit'],\n",
    "            scale_limit=aug_config[2]['scale_limit'],\n",
    "            rotate_limit=aug_config[2]['rotate_limit'],\n",
    "            p=aug_config[2]['p']\n",
    "        ),\n",
    "        A.ColorJitter(\n",
    "            brightness=aug_config[3]['brightness'],\n",
    "            contrast=aug_config[3]['contrast'],\n",
    "            saturation=aug_config[3]['saturation'],\n",
    "            hue=aug_config[3]['hue'],\n",
    "            p=aug_config[3]['p']\n",
    "        ),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=aug_config[4]['max_holes'],\n",
    "            max_height=aug_config[4]['max_height'],\n",
    "            max_width=aug_config[4]['max_width'],\n",
    "            p=aug_config[4]['p']\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def processImage(img_path, output_dir, transform, save_augmented=True):\n",
    "    \"\"\"Process and save a single image with augmentation\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=img_np)['image']\n",
    "        \n",
    "        if save_augmented:\n",
    "            # Save augmented image\n",
    "            aug_name = f\"{Path(img_path).stem}_aug.jpg\"\n",
    "            aug_path = os.path.join(output_dir, aug_name)\n",
    "            Image.fromarray(augmented).save(aug_path, quality=CONFIG['JPEG_QUALITY'], optimize=True)\n",
    "            return True\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def transformImagesFromDirectory(species_name, data_dir, metadata_path=None, save_augmented=True):\n",
    "    start_time = time.time()\n",
    "    species_dir = os.path.join(data_dir, species_name.replace(\" \", \"_\"))\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(data_dir)\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'original_count': 0,\n",
    "        'augmented_saved': 0,\n",
    "        'start_time': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    }\n",
    "\n",
    "    # Create output directory\n",
    "    if save_augmented:\n",
    "        output_dir = os.path.join(data_dir, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = species_dir\n",
    "\n",
    "    # Get augmentation pipeline\n",
    "    transform = getAugmentation()\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = [os.path.join(species_dir, f) for f in os.listdir(species_dir) \n",
    "                  if os.path.isfile(os.path.join(species_dir, f)) and not f.endswith(\".json\")]\n",
    "    \n",
    "    stats['original_count'] = len(image_paths)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: processImage(p, output_dir, transform, save_augmented),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Augmenting {species_name}\"\n",
    "        ))\n",
    "    \n",
    "    stats['augmented_saved'] = sum(results)\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().isoformat(),\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "\n",
    "    updateLogging(metadata_path, {\"augmentation\": {species_name: stats}})\n",
    "    return stats['augmented_saved']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d9cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return min(img.size) >= IMG_SIZE_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def getPhash(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return imagehash.phash(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanData(species_name, dir, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    species_path = os.path.join(dir, species_name.replace(\" \", \"_\"))\n",
    "    hash_db = []\n",
    "    stats = {\n",
    "        'removed': 0,\n",
    "        'remaining': 0,\n",
    "        'duplicates': 0,\n",
    "        'invalid': 0\n",
    "    }\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = list(Path(species_path).glob(\"*.*\"))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: (p, isValidImage(p), getPhash(p)),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Cleaning {species_name}\"\n",
    "        ))\n",
    "\n",
    "    # Process results\n",
    "    for img_path, is_valid, phash in results:\n",
    "        if not is_valid:\n",
    "            os.remove(img_path)\n",
    "            stats['invalid'] += 1\n",
    "            stats['removed'] += 1\n",
    "        elif phash is None:\n",
    "            os.remove(img_path)\n",
    "            stats['removed'] += 1\n",
    "        elif any(phash - existing < HASH_THRESHOLD for existing in hash_db):\n",
    "            os.remove(img_path)\n",
    "            stats['duplicates'] += 1\n",
    "            stats['removed'] += 1\n",
    "        else:\n",
    "            hash_db.append(phash)\n",
    "            stats['remaining'] += 1\n",
    "\n",
    "    stats.update({\n",
    "        'time_seconds': time.time() - start_time,\n",
    "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    })\n",
    "    updateLogging(metadata_path, {\"cleaning\": {species_name: stats}})\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize log if not provided\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    # Data collection structures\n",
    "    images = []\n",
    "    labels = []\n",
    "    species_counts = defaultdict(int)\n",
    "    for species_idx, (species_name, _) in enumerate(species_keys.items()):\n",
    "        species_dir = os.path.join(DATA_DIR, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        if not os.path.exists(species_dir):\n",
    "            continue\n",
    "\n",
    "        for img_name in os.listdir(species_dir):\n",
    "            img_path = os.path.join(species_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                images.append(np.array(img))\n",
    "                labels.append(species_idx)\n",
    "                species_counts[species_name] += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Convert to numpy arrays for HDF5\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # 1. First split: 70% train, 30% temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3,  # 30% for val + test\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 2. Second split: 15% val, 15% test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,  # Split 30% into equal 15% parts\n",
    "        stratify=y_temp,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Add fold column for cross-validation\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=CONFIG['N_SPLITS'], \n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create HDF5 dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "    h5_path = os.path.join(DATA_DIR, f\"dataset_{timestamp}.h5\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'w') as hf:\n",
    "        # Test set\n",
    "        test_group = hf.create_group('test')\n",
    "        test_group.create_dataset('X_test', data=X_test, \n",
    "                                compression=CONFIG['COMPRESSION'], \n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        test_group.create_dataset('y_test', data=y_test,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        \n",
    "        # Train set\n",
    "        train_group = hf.create_group('train')\n",
    "        train_group.create_dataset('X_train', data=X_train,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        train_group.create_dataset('y_train', data=y_train,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Validation set\n",
    "        val_group = hf.create_group('val')\n",
    "        val_group.create_dataset('X_val', data=X_val,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        val_group.create_dataset('y_val', data=y_val,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Cross-validation splits\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            fold_group = cv_group.create_group(f'fold_{fold + 1}')\n",
    "            fold_group.create_dataset('X_train', data=X_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Save metadata\n",
    "        hf.attrs['species'] = json.dumps(list(species_keys.keys()))\n",
    "        hf.attrs['image_size'] = json.dumps(CONFIG['IMG_SIZE'])\n",
    "        hf.attrs['augmentation'] = json.dumps(CONFIG['AUGMENTATION'])\n",
    "        hf.attrs['creation_time'] = timestamp\n",
    "    \n",
    "    # Update metadata log\n",
    "    dataset_stats = {\n",
    "        'total_images': len(images),\n",
    "        'species_counts': dict(species_counts),\n",
    "        'h5_path': h5_path,\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'compression': CONFIG['COMPRESSION'],\n",
    "        'compression_level': CONFIG['COMPRESSION_LEVEL'],\n",
    "        'processing_time_seconds': time.time() - start_time,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    updateLogging(metadata_path, {\n",
    "        \"dataset_stats\": dataset_stats\n",
    "    })\n",
    "\n",
    "    print(f\"Dataset created with multiple formats:\")\n",
    "    print(f\"- HDF5 file: {h5_path}\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9d9571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset creation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Carduelis carduelis: 100%|██████████| 599/599 [00:01<00:00, 572.76it/s]\n",
      "Cleaning Ciconia ciconia: 100%|██████████| 600/600 [00:01<00:00, 548.17it/s]\n",
      "Cleaning Columba livia: 100%|██████████| 600/600 [00:00<00:00, 611.61it/s]\n",
      "Cleaning Delichon urbicum: 100%|██████████| 596/596 [00:00<00:00, 661.65it/s]\n",
      "Cleaning Emberiza calandra: 100%|██████████| 600/600 [00:00<00:00, 635.53it/s]\n",
      "Cleaning Hirundo rustica: 100%|██████████| 597/597 [00:00<00:00, 615.42it/s]\n",
      "Cleaning Passer domesticus: 100%|██████████| 600/600 [00:01<00:00, 579.83it/s]\n",
      "Cleaning Serinus serinus: 100%|██████████| 600/600 [00:01<00:00, 458.92it/s]\n",
      "Cleaning Streptopelia decaocto: 100%|██████████| 600/600 [00:01<00:00, 580.45it/s]\n",
      "Cleaning Sturnus unicolor: 100%|██████████| 600/600 [00:00<00:00, 610.08it/s]\n",
      "Cleaning Turdus merula: 100%|██████████| 596/596 [00:01<00:00, 554.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with multiple formats:\n",
      "- HDF5 file: full_image_dataset\\dataset_20250524_131708.h5\n",
      "Total processing time: 144.13 seconds\n",
      "Tasks completed\n"
     ]
    }
   ],
   "source": [
    "SKIP_DOWNLOAD = True\n",
    "#dir = DATA_DIR\n",
    "dir = f\"{DATA_DIR}/{AUGMENTED_DIR}\"\n",
    "\n",
    "print(\"Initiating dataset creation...\")\n",
    "metadata_path = initLogging(DATA_DIR)\n",
    "for species in species_keys.keys():\n",
    "    if not SKIP_DOWNLOAD:\n",
    "        downloadImages(species, DATA_DIR, limit=600, metadata_path=metadata_path)\n",
    "    #transformImagesFromDirectory(species, DATA_DIR, metadata_path, True)\n",
    "    cleanData(species, dir, metadata_path)\n",
    "createDataset(metadata_path)\n",
    "print(\"Tasks completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
