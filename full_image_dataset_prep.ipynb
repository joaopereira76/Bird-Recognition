{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa331ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-Master\\AppData\\Local\\Temp\\ipykernel_4516\\2434243120.py:23: DeprecationWarning: The module `pyinaturalist.node_api` is deprecated; please use `from pyinaturalist import ...`\n",
      "  from pyinaturalist.node_api import get_observations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import requests\n",
    "import h5py\n",
    "import psutil\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "from pygbif import occurrences\n",
    "from pyinaturalist.node_api import get_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6ff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"full_image_dataset\"          # Expected input dir: species_name/*.jpg\n",
    "AUGMENTED_DIR = \"augmented_dataset\"     # Augmented images will be saved here\n",
    "IMG_SIZE_THRESHOLD = 200              # Min resolution (px)\n",
    "HASH_THRESHOLD = 8                    # Duplicate threshold using phash\n",
    "\n",
    "species_keys = {\n",
    "    \"Carduelis carduelis\": 2494686,\n",
    "    \"Ciconia ciconia\": 2481912,\n",
    "    \"Columba livia\": 2495414,\n",
    "    \"Delichon urbicum\": 2489214,\n",
    "    \"Emberiza calandra\":7634625,\n",
    "    \"Hirundo rustica\": 7192162,\n",
    "    \"Passer domesticus\": 5231190,\n",
    "    \"Serinus serinus\":2494200,\n",
    "    \"Streptopelia decaocto\": 2495696,\n",
    "    \"Sturnus unicolor\":2489104,\n",
    "    \"Turdus merula\": 6171845   \n",
    "}\n",
    "\n",
    "\"\"\"CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.15,\n",
    "    'N_SPLITS': 3,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 6,\n",
    "    'SAVE_AS_JPEG': True,\n",
    "    'JPEG_QUALITY': 80,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomHorzontalFlip', 'p': 0.5},\n",
    "            {'name': 'RandomRotation', 'degrees': 20},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1}\n",
    "        ]\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.15,\n",
    "    'N_SPLITS': 5,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 6,\n",
    "    'SAVE_AS_JPEG': True,\n",
    "    'JPEG_QUALITY': 80,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomResizedCrop','size':(224,224) , 'scale': (0.8, 1.0)},\n",
    "            {'name': 'HorizontalFlip', 'p': 0.5},\n",
    "            {'name': 'ShiftScaleRotate', 'shift_limit': 0.05, 'scale_limit': 0.1, 'rotate_limit': 20, 'p': 0.7},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'hue': 0.05, 'p': 0.8},\n",
    "            {'name': 'CoarseDropout', 'max_holes':1, 'max_height': 48, 'max_width': 48, 'p': 0.4},\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afd675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"os\": platform.system(),\n",
    "        \"os_version\": platform.release(),\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"cpu_cores\": psutil.cpu_count(logical=False),\n",
    "        \"ram_total_gb\": round(mem.total / (1024**3), 2),\n",
    "        \"ram_available_gb\": round(mem.available / (1024**3), 2),\n",
    "        \"python_version\": platform.python_version()\n",
    "    }\n",
    "\n",
    "def initLogging(output_dir):\n",
    "    metadata = {\n",
    "        \"config\": CONFIG,\n",
    "        \"system\": getSystemInfo(),\n",
    "        \"download\": {},\n",
    "        \"cleaning\": {},\n",
    "        \"dataset_stats\": {},\n",
    "    }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, f\"dataset_prep_{datetime.now().strftime(\"%Y%m%d\")}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata_path\n",
    "\n",
    "def updateLogging(metadata_path, updates):\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return initLogging(os.path.dirname(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    metadata.update(updates)\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52038183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadImages(species_name, output_dir, limit=500, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    output_dir = os.path.join(DATA_DIR, species_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    print(f\"\\nDownloading images for: {species_name}\")\n",
    "    stats = {\n",
    "        'iNaturalist': 0,\n",
    "        'GBIF': 0,\n",
    "        'start_time': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #iNaturalist download\n",
    "        stats['iNaturalist'] = downloadImages_INaturalist(species_name, output_dir, limit)\n",
    "        \n",
    "        # GBIF download\n",
    "        current_count = stats['iNaturalist']\n",
    "        stats['GBIF'] = downloadImages_GBIF(species_name, current_count, output_dir, limit - current_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "    \n",
    "    # Update metadata\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        'total_downloaded': stats['iNaturalist'] + stats['GBIF'],\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    updateLogging(metadata_path, {\"download\": {species_name: stats}})\n",
    "    print(f\"Total images downloaded for {species_name}: {stats['total_downloaded']}\")\n",
    "    return stats['total_downloaded']\n",
    "\n",
    "def downloadImages_INaturalist(species_name, output_dir, limit=500):\n",
    "    results = get_observations(\n",
    "        taxon_name=species_name,\n",
    "        per_page=limit,\n",
    "        quality_grade=\"research\",\n",
    "        media_type=\"photo\",\n",
    "        license=[\"CC-BY\",\"CC-BY-NC\"] \n",
    "    )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for photo in obs.get(\"photos\",[]):\n",
    "            url = photo.get(\"url\", \"\").replace(\"square\", \"original\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{images_downloaded}.{image_ext}\"\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Downloaded {images_downloaded} images from iNaturalist for {species_name}\")\n",
    "    return images_downloaded\n",
    "\n",
    "def downloadImages_GBIF(species_name, downloadedValue, output_dir, limit=500):\n",
    "    results = occurrences.search(\n",
    "            taxonKey=species_keys[species_name],\n",
    "            mediaType=\"StillImage\",\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for media in obs.get(\"media\",[]):\n",
    "            url = media.get(\"identifier\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{downloadedValue + images_downloaded}.{image_ext}\"\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break   \n",
    "    print(f\"\\nDownloaded {images_downloaded} images from GBIF for {species_name}\")\n",
    "    return images_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d3d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAugmentation():\n",
    "    aug_config = CONFIG['AUGMENTATION']['train']\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(\n",
    "            size=aug_config[0]['size'],\n",
    "            scale=aug_config[0]['scale'],\n",
    "        ),\n",
    "        A.HorizontalFlip(p=aug_config[1]['p']),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=aug_config[2]['shift_limit'],\n",
    "            scale_limit=aug_config[2]['scale_limit'],\n",
    "            rotate_limit=aug_config[2]['rotate_limit'],\n",
    "            p=aug_config[2]['p']\n",
    "        ),\n",
    "        A.ColorJitter(\n",
    "            brightness=aug_config[3]['brightness'],\n",
    "            contrast=aug_config[3]['contrast'],\n",
    "            saturation=aug_config[3]['saturation'],\n",
    "            hue=aug_config[3]['hue'],\n",
    "            p=aug_config[3]['p']\n",
    "        ),\n",
    "        A.CoarseDropout(\n",
    "            max_holes=aug_config[4]['max_holes'],\n",
    "            max_height=aug_config[4]['max_height'],\n",
    "            max_width=aug_config[4]['max_width'],\n",
    "            p=aug_config[4]['p']\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def processImage(img_path, output_dir, transform, save_augmented=True):\n",
    "    \"\"\"Process and save a single image with augmentation\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=img_np)['image']\n",
    "        \n",
    "        if save_augmented:\n",
    "            # Save augmented image\n",
    "            aug_name = f\"{Path(img_path).stem}_aug.jpg\"\n",
    "            aug_path = os.path.join(output_dir, aug_name)\n",
    "            Image.fromarray(augmented).save(aug_path, quality=CONFIG['JPEG_QUALITY'], optimize=True)\n",
    "            return True\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def transformImagesFromDirectory(species_name, data_dir, metadata_path=None, save_augmented=True):\n",
    "    start_time = time.time()\n",
    "    species_dir = os.path.join(data_dir, species_name.replace(\" \", \"_\"))\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(data_dir)\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'original_count': 0,\n",
    "        'augmented_saved': 0,\n",
    "        'start_time': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    }\n",
    "\n",
    "    # Create output directory\n",
    "    if save_augmented:\n",
    "        output_dir = os.path.join(data_dir, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    else:\n",
    "        output_dir = species_dir\n",
    "\n",
    "    # Get augmentation pipeline\n",
    "    transform = getAugmentation()\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = [os.path.join(species_dir, f) for f in os.listdir(species_dir) \n",
    "                  if os.path.isfile(os.path.join(species_dir, f)) and not f.endswith(\".json\")]\n",
    "    \n",
    "    stats['original_count'] = len(image_paths)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: processImage(p, output_dir, transform, save_augmented),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Augmenting {species_name}\"\n",
    "        ))\n",
    "    \n",
    "    stats['augmented_saved'] = sum(results)\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().isoformat(),\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "\n",
    "    updateLogging(metadata_path, {\"augmentation\": {species_name: stats}})\n",
    "    return stats['augmented_saved']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d9cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return min(img.size) >= IMG_SIZE_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def getPhash(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return imagehash.phash(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanData(species_name, dir, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    species_path = os.path.join(dir, species_name.replace(\" \", \"_\"))\n",
    "    hash_db = []\n",
    "    stats = {\n",
    "        'removed': 0,\n",
    "        'remaining': 0,\n",
    "        'duplicates': 0,\n",
    "        'invalid': 0\n",
    "    }\n",
    "\n",
    "    # Process images in parallel\n",
    "    image_paths = list(Path(species_path).glob(\"*.*\"))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda p: (p, isValidImage(p), getPhash(p)),\n",
    "                image_paths\n",
    "            ),\n",
    "            total=len(image_paths),\n",
    "            desc=f\"Cleaning {species_name}\"\n",
    "        ))\n",
    "\n",
    "    # Process results\n",
    "    for img_path, is_valid, phash in results:\n",
    "        if not is_valid:\n",
    "            os.remove(img_path)\n",
    "            stats['invalid'] += 1\n",
    "            stats['removed'] += 1\n",
    "        elif phash is None:\n",
    "            os.remove(img_path)\n",
    "            stats['removed'] += 1\n",
    "        elif any(phash - existing < HASH_THRESHOLD for existing in hash_db):\n",
    "            os.remove(img_path)\n",
    "            stats['duplicates'] += 1\n",
    "            stats['removed'] += 1\n",
    "        else:\n",
    "            hash_db.append(phash)\n",
    "            stats['remaining'] += 1\n",
    "\n",
    "    stats.update({\n",
    "        'time_seconds': time.time() - start_time,\n",
    "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    })\n",
    "    updateLogging(metadata_path, {\"cleaning\": {species_name: stats}})\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize log if not provided\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    # Create directories\n",
    "    processed_dir = os.path.join(DATA_DIR, \"processed_images_224\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Data collection structures\n",
    "    images = []\n",
    "    labels = []\n",
    "    rows = []\n",
    "    species_counts = defaultdict(int)\n",
    "        \n",
    "    # Process images\n",
    "    for idx, specie in enumerate(species_keys):\n",
    "        specie_dir = os.path.join(DATA_DIR, specie.replace(\" \", \"_\"))\n",
    "        specie_out_dir = os.path.join(processed_dir, specie.replace(\" \", \"_\"))\n",
    "        os.makedirs(specie_out_dir, exist_ok=True)\n",
    "        \n",
    "        for img_name in os.listdir(specie_dir):\n",
    "            img_path = os.path.join(specie_dir, img_name)\n",
    "            out_path = os.path.join(specie_out_dir, img_name)\n",
    "            \n",
    "            try:\n",
    "                # Process and save image\n",
    "                img = Image.open(img_path).convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                img.save(out_path, optimize=True, quality=85)\n",
    "                \n",
    "                # For HDF5\n",
    "                images.append(np.array(img))\n",
    "                labels.append(idx)\n",
    "                \n",
    "                # For CSV\n",
    "                rows.append({\n",
    "                    \"filepath\": out_path,\n",
    "                    \"label_idx\": idx,\n",
    "                    \"species\": specie\n",
    "                })\n",
    "                \n",
    "                species_counts[specie] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays for HDF5\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['TEST_SIZE'], stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create CSV dataset\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(DATA_DIR, f\"dataset_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    \n",
    "    # Add fold column for cross-validation\n",
    "    df['fold'] = -1\n",
    "    skf = StratifiedShuffleSplit(\n",
    "        n_splits=CONFIG['N_SPLITS'],\n",
    "        test_size=CONFIG['TEST_SIZE'],\n",
    "        train_size=CONFIG['TRAIN_SIZE'],\n",
    "        random_state=42\n",
    "    )\n",
    "    for fold_idx, (_, val_idx) in enumerate(skf.split(df['filepath'], df['label_idx'])):\n",
    "        df.loc[val_idx, 'fold'] = fold_idx\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Create HDF5 dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    h5_path = os.path.join(DATA_DIR, f\"dataset_{timestamp}.h5\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'w') as hf:\n",
    "        # Test set\n",
    "        test_group = hf.create_group('test')\n",
    "        test_group.create_dataset('X_test', data=X_test, \n",
    "                                compression=CONFIG['COMPRESSION'], \n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        test_group.create_dataset('y_test', data=y_test,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Cross-validation splits\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            fold_group = cv_group.create_group(f'fold_{fold + 1}')\n",
    "            fold_group.create_dataset('X_train', data=X_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Save metadata\n",
    "        hf.attrs['species'] = json.dumps(list(species_keys.keys()))\n",
    "        hf.attrs['image_size'] = json.dumps(CONFIG['IMG_SIZE'])\n",
    "        hf.attrs['augmentation'] = json.dumps(CONFIG['AUGMENTATION'])\n",
    "        hf.attrs['creation_time'] = timestamp\n",
    "        hf.attrs['csv_reference'] = csv_path\n",
    "    \n",
    "    # Update metadata log\n",
    "    dataset_stats = {\n",
    "        'total_images': len(images),\n",
    "        'species_counts': dict(species_counts),\n",
    "        'h5_path': h5_path,\n",
    "        'csv_path': csv_path,\n",
    "        'processed_images_dir': processed_dir,\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'compression': CONFIG['COMPRESSION'],\n",
    "        'compression_level': CONFIG['COMPRESSION_LEVEL'],\n",
    "        'processing_time_seconds': time.time() - start_time,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    updateLogging(metadata_path, {\n",
    "        \"dataset_stats\": dataset_stats\n",
    "    })\n",
    "\n",
    "    print(f\"Dataset created with multiple formats:\")\n",
    "    print(f\"- HDF5 file: {h5_path}\")\n",
    "    print(f\"- CSV metadata: {csv_path}\")\n",
    "    print(f\"- Processed images: {processed_dir}\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\"\"\"\n",
    "\n",
    "def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_species = []\n",
    "    filepaths = []\n",
    "    species_counts = defaultdict(int)\n",
    "\n",
    "    # Load all image data\n",
    "    for species_idx, (species_name, _) in enumerate(species_keys.items()):\n",
    "        species_dir = os.path.join(DATA_DIR, AUGMENTED_DIR, species_name.replace(\" \", \"_\"))\n",
    "        if not os.path.exists(species_dir):\n",
    "            continue\n",
    "\n",
    "        for img_name in os.listdir(species_dir):\n",
    "            img_path = os.path.join(species_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                all_images.append(np.array(img))\n",
    "                all_labels.append(species_idx)\n",
    "                all_species.append(species_name)  # used as group label\n",
    "                filepaths.append(f\"{species_name.replace(' ', '_')}/{img_name}\")\n",
    "                species_counts[species_name] += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Convert to arrays\n",
    "    X = np.array(all_images)\n",
    "    y = np.array(all_labels)\n",
    "    groups = np.array(all_species)\n",
    "\n",
    "    # First split off test set (15%)\n",
    "    X_temp, X_test, y_temp, y_test, groups_temp, groups_test = train_test_split(\n",
    "        X, y, groups,\n",
    "        test_size=CONFIG['TEST_SIZE'],\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Prepare metadata DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'species': [list(species_keys.keys())[i] for i in y_temp],\n",
    "        'label_idx': y_temp,\n",
    "        'filepath': [filepaths[i] for i in range(len(filepaths)) if i < len(X_temp)],\n",
    "        'group': groups_temp\n",
    "    })\n",
    "\n",
    "    # Apply StratifiedGroupKFold on remaining 85%\n",
    "    skf = StratifiedGroupKFold(n_splits=CONFIG['N_SPLITS'])\n",
    "    df['fold'] = -1\n",
    "    for fold_idx, (_, val_idx) in enumerate(skf.split(df, df['label_idx'], groups=df['group'])):\n",
    "        df.loc[val_idx, 'fold'] = fold_idx\n",
    "\n",
    "    # Save CSV metadata\n",
    "    timestamp = datetime.now().strftime('%Y%m%d')\n",
    "    csv_path = os.path.join(DATA_DIR, f\"dataset_metadata_{timestamp}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save HDF5 dataset\n",
    "    h5_path = os.path.join(DATA_DIR, f\"dataset_{timestamp}.h5\")\n",
    "    with h5py.File(h5_path, 'w') as hf:\n",
    "        # Test set\n",
    "        test_group = hf.create_group('test')\n",
    "        test_group.create_dataset('X_test', data=X_test,\n",
    "                                   compression=CONFIG['COMPRESSION'],\n",
    "                                   compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        test_group.create_dataset('y_test', data=y_test,\n",
    "                                   compression=CONFIG['COMPRESSION'],\n",
    "                                   compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Cross-validation folds\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        for fold in range(CONFIG['N_SPLITS']):\n",
    "            fold_indices = df[df['fold'] == fold].index.values\n",
    "            train_indices = df[df['fold'] != fold].index.values\n",
    "\n",
    "            fold_group = cv_group.create_group(f'fold_{fold+1}')\n",
    "            fold_group.create_dataset('X_train', data=X_temp[train_indices],\n",
    "                                      compression=CONFIG['COMPRESSION'],\n",
    "                                      compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y_temp[train_indices],\n",
    "                                      compression=CONFIG['COMPRESSION'],\n",
    "                                      compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X_temp[fold_indices],\n",
    "                                      compression=CONFIG['COMPRESSION'],\n",
    "                                      compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y_temp[fold_indices],\n",
    "                                      compression=CONFIG['COMPRESSION'],\n",
    "                                      compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Metadata as attributes\n",
    "        hf.attrs['species'] = json.dumps(list(species_keys.keys()))\n",
    "        hf.attrs['image_size'] = json.dumps(CONFIG['IMG_SIZE'])\n",
    "        hf.attrs['augmentation'] = json.dumps(CONFIG['AUGMENTATION'])\n",
    "        hf.attrs['creation_time'] = timestamp\n",
    "        hf.attrs['csv_reference'] = csv_path\n",
    "\n",
    "    # Update JSON metadata\n",
    "    dataset_stats = {\n",
    "        'total_images': len(all_images),\n",
    "        'species_counts': dict(species_counts),\n",
    "        'h5_path': h5_path,\n",
    "        'csv_path': csv_path,\n",
    "        'train_samples': len(X_temp),\n",
    "        'test_samples': len(X_test),\n",
    "        'processing_time_seconds': time.time() - start_time,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "\n",
    "    updateLogging(metadata_path, {\"dataset_stats\": dataset_stats})\n",
    "    print(f\"Dataset created with {len(all_images)} images\")\n",
    "    print(f\"- HDF5: {h5_path}\")\n",
    "    print(f\"- CSV: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9d9571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\Bird-Recognition\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\PC-Master\\AppData\\Local\\Temp\\ipykernel_9712\\3651381001.py:22: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset creation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Carduelis carduelis: 100%|██████████| 590/590 [00:05<00:00, 116.38it/s]\n",
      "Cleaning Carduelis carduelis: 100%|██████████| 600/600 [00:00<00:00, 850.63it/s]\n",
      "Augmenting Ciconia ciconia: 100%|██████████| 592/592 [00:05<00:00, 108.71it/s]\n",
      "Cleaning Ciconia ciconia: 100%|██████████| 600/600 [00:00<00:00, 842.88it/s]\n",
      "Augmenting Columba livia: 100%|██████████| 594/594 [00:05<00:00, 110.02it/s]\n",
      "Cleaning Columba livia: 100%|██████████| 600/600 [00:00<00:00, 892.05it/s]\n",
      "Augmenting Delichon urbicum: 100%|██████████| 571/571 [00:04<00:00, 135.92it/s]\n",
      "Cleaning Delichon urbicum: 100%|██████████| 597/597 [00:00<00:00, 958.84it/s]\n",
      "Augmenting Emberiza calandra: 100%|██████████| 591/591 [00:04<00:00, 143.75it/s]\n",
      "Cleaning Emberiza calandra: 100%|██████████| 600/600 [00:00<00:00, 899.32it/s]\n",
      "Augmenting Hirundo rustica: 100%|██████████| 581/581 [00:04<00:00, 129.57it/s]\n",
      "Cleaning Hirundo rustica: 100%|██████████| 600/600 [00:00<00:00, 897.08it/s]\n",
      "Augmenting Passer domesticus: 100%|██████████| 597/597 [00:05<00:00, 116.54it/s]\n",
      "Cleaning Passer domesticus: 100%|██████████| 600/600 [00:00<00:00, 891.37it/s]\n",
      "Augmenting Serinus serinus: 100%|██████████| 590/590 [00:04<00:00, 128.06it/s]\n",
      "Cleaning Serinus serinus: 100%|██████████| 600/600 [00:00<00:00, 878.41it/s]\n",
      "Augmenting Streptopelia decaocto: 100%|██████████| 595/595 [00:04<00:00, 121.00it/s]\n",
      "Cleaning Streptopelia decaocto: 100%|██████████| 600/600 [00:00<00:00, 881.07it/s]\n",
      "Augmenting Sturnus unicolor: 100%|██████████| 594/594 [00:04<00:00, 134.47it/s]\n",
      "Cleaning Sturnus unicolor: 100%|██████████| 600/600 [00:00<00:00, 881.02it/s]\n",
      "Augmenting Turdus merula: 100%|██████████| 569/569 [00:04<00:00, 126.00it/s]\n",
      "Cleaning Turdus merula: 100%|██████████| 596/596 [00:00<00:00, 891.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 6583 images\n",
      "- HDF5: full_image_dataset\\dataset_20250523.h5\n",
      "- CSV: full_image_dataset\\dataset_metadata_20250523.csv\n",
      "Tasks completed\n"
     ]
    }
   ],
   "source": [
    "SKIP_DOWNLOAD = True\n",
    "#dir = DATA_DIR\n",
    "dir = f\"{DATA_DIR}/{AUGMENTED_DIR}\"\n",
    "\n",
    "print(\"Initiating dataset creation...\")\n",
    "metadata_path = initLogging(DATA_DIR)\n",
    "for species in species_keys.keys():\n",
    "    if not SKIP_DOWNLOAD:\n",
    "        downloadImages(species, DATA_DIR, limit=600, metadata_path=metadata_path)\n",
    "    transformImagesFromDirectory(species, DATA_DIR, metadata_path, True)\n",
    "    cleanData(species, dir, metadata_path)\n",
    "createDataset(metadata_path)\n",
    "print(\"Tasks completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
