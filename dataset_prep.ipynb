{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e34d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import requests\n",
    "import h5py\n",
    "import psutil\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pygbif import occurrences\n",
    "from pyinaturalist.node_api import get_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9265288",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"full_image_dataset\"          # Expected input dir: species_name/*.jpg\n",
    "IMG_SIZE_THRESHOLD = 200              # Min resolution (px)\n",
    "HASH_THRESHOLD = 8                    # Duplicate threshold using phash\n",
    "\n",
    "species_keys = {\n",
    "    \"Carduelis carduelis\": 2494686,\n",
    "    \"Ciconia ciconia\": 2481912,\n",
    "    \"Columba livia\": 2495414,\n",
    "    \"Delichon urbicum\": 2489214,\n",
    "    \"Emberiza calandra\":7634625,\n",
    "    \"Hirundo rustica\": 7192162,\n",
    "    \"Passer domesticus\": 5231190,\n",
    "    \"Serinus serinus\":2494200,\n",
    "    \"Streptopelia decaocto\": 2495696,\n",
    "    \"Sturnus unicolor\":2489104,\n",
    "    \"Turdus merula\": 6171845   \n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.15,\n",
    "    'N_SPLITS': 5,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 6,\n",
    "    'SAVE_AS_JPEG': True,\n",
    "    'JPEG_QUALITY': 80,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomResizedCrop', 'size': (224, 224), 'scale': (0.8, 1.0)},\n",
    "            {'name': 'RandomHorizontalFlip', 'p': 0.5},\n",
    "            {'name': 'RandomRotation', 'degrees': 15},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'hue': 0.05, 'p': 0.8},\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b52404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "        \"os\": platform.system(),\n",
    "        \"os_version\": platform.release(),\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"ram_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"python_version\": platform.python_version()\n",
    "    }\n",
    "\n",
    "def initLogging(output_dir):\n",
    "    metadata = {\n",
    "        \"config\": CONFIG,\n",
    "        \"system\": getSystemInfo(),\n",
    "        \"download\": {},\n",
    "        \"cleaning\": {},\n",
    "        \"augmentation\": {},\n",
    "        \"dataset_stats\": {},\n",
    "        \"processing_times\": {}\n",
    "    }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, f\"dataset_prep_{datetime.now().strftime('%Y%m%d')}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata_path\n",
    "\n",
    "def updateLogging(metadata_path, updates):\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return initLogging(os.path.dirname(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    metadata.update(updates)\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2301b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(img):\n",
    "    transforms_list = []\n",
    "    for aug in CONFIG['AUGMENTATION']['train']:\n",
    "        if aug['name'] == 'RandomResizedCrop':\n",
    "            transforms_list.append(transforms.RandomResizedCrop(size=aug['size'], scale=aug['scale']))\n",
    "        elif aug['name'] == 'RandomHorizontalFlip':\n",
    "            transforms_list.append(transforms.RandomHorizontalFlip(p=aug['p']))\n",
    "        elif aug['name'] == 'RandomRotation':\n",
    "            transforms_list.append(transforms.RandomRotation(degrees=aug['degrees']))\n",
    "        elif aug['name'] == 'ColorJitter':\n",
    "            transforms_list.append(transforms.ColorJitter(brightness=aug['brightness'], contrast=aug['contrast'],\n",
    "                                                          saturation=aug['saturation'], hue=aug['hue']))\n",
    "    return transforms.Compose(transforms_list)(img)\n",
    "\n",
    "\n",
    "def downloadImages(species_name, output_dir, limit=500, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    output_dir = os.path.join(DATA_DIR, species_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    print(f\"\\nDownloading images for: {species_name}\")\n",
    "    stats = {\n",
    "        'iNaturalist': 0,\n",
    "        'GBIF': 0,\n",
    "        'start_time': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #iNaturalist download\n",
    "        stats['iNaturalist'] = downloadImages_INaturalist(species_name, output_dir, limit)\n",
    "        \n",
    "        # GBIF download\n",
    "        current_count = stats['iNaturalist']\n",
    "        stats['GBIF'] = downloadImages_GBIF(species_name, current_count, output_dir, limit - current_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "    \n",
    "    # Update metadata\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "        'total_downloaded': stats['iNaturalist'] + stats['GBIF'],\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    updateLogging(metadata_path, {\"download\": {species_name: stats}})\n",
    "    print(f\"Total images downloaded for {species_name}: {stats['total_downloaded']}\")\n",
    "    return stats['total_downloaded']\n",
    "\n",
    "def downloadImages_INaturalist(species_name, output_dir, limit=500):\n",
    "    results = get_observations(\n",
    "        taxon_name=species_name,\n",
    "        per_page=limit,\n",
    "        quality_grade=\"research\",\n",
    "        media_type=\"photo\",\n",
    "        license=[\"CC-BY\",\"CC-BY-NC\"] \n",
    "    )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for photo in obs.get(\"photos\",[]):\n",
    "            url = photo.get(\"url\", \"\").replace(\"square\", \"original\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                augmented_img = apply_augmentation(img)\n",
    "                image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                filename = f\"{species_name.replace(' ', '_')}_{images_downloaded}.{image_ext}\"\n",
    "                \n",
    "                augmented_img.save(os.path.join(output_dir, filename), quality=CONFIG['JPEG_QUALITY'])\n",
    "                images_downloaded += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Download error for {url}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing or saving {url}: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Downloaded {images_downloaded} images from iNaturalist for {species_name}\")\n",
    "    return images_downloaded\n",
    "\n",
    "def downloadImages_GBIF(species_name, downloadedValue, output_dir, limit=500):\n",
    "    images_downloaded = 0\n",
    "    try:\n",
    "        result = occurrences.search(\n",
    "            taxonKey=species_keys[species_name],\n",
    "            mediaType=\"StillImage\",\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        for idx, occ in enumerate(result.get(\"results\", [])):\n",
    "            for media in occ.get(\"media\",[]):\n",
    "                if media.get(\"type\") != \"StillImage\":\n",
    "                    continue\n",
    "                    \n",
    "                imgURL = media.get(\"identifier\")\n",
    "                if not imgURL:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    response = requests.get(imgURL, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    augmented_img = apply_augmentation(img)\n",
    "                    ext = 'jpg' if 'jpeg' in response.headers.get('content-type', '') else 'png'\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{downloadedValue + images_downloaded}.{ext}\"\n",
    "\n",
    "                    augmented_img.save(os.path.join(output_dir, filename), quality=CONFIG['JPEG_QUALITY'])\n",
    "                    images_downloaded += 1\n",
    "                    print(f\"Downloaded image {images_downloaded}/{limit}\", end='\\r')\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"\\nDownload error for image {idx} for {species_name}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing or saving image {idx} for {species_name}: {e}\")\n",
    "                    continue\n",
    "                if images_downloaded >= limit:\n",
    "                    break\n",
    "            if images_downloaded >= limit:\n",
    "                break \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching occurrences for {species_name}: {e}\")\n",
    "    print(f\"\\nDownloaded {images_downloaded} images from GBIF for {species_name}\")\n",
    "    return images_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fc53484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return min(img.size) >= IMG_SIZE_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def getPhash(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return imagehash.phash(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanData(species_name, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    # Initialize hash_db as a dictionary instead of a list\n",
    "    hash_db = defaultdict(list)\n",
    "    total_removed = 0\n",
    "    imgs_to_remove = []\n",
    "    valid_images_count = 0\n",
    "\n",
    "    print(f\"Starting image cleaning for: {species_name}\")\n",
    "\n",
    "    species_path = Path(DATA_DIR) / species_name.replace(\" \", \"_\")\n",
    "    if not species_path.is_dir():\n",
    "        print(f\"Directory {species_path} does not exist.\")\n",
    "        return 0\n",
    "\n",
    "    # Get all image paths first\n",
    "    img_paths = list(species_path.glob(\"*.*\"))\n",
    "    \n",
    "    for img_path in tqdm(img_paths, desc=f\"Cleaning {species_name}\"):\n",
    "        if not isValidImage(img_path):\n",
    "            imgs_to_remove.append(img_path)\n",
    "            continue\n",
    "\n",
    "        phash = getPhash(img_path)\n",
    "        if phash is None:\n",
    "            imgs_to_remove.append(img_path)\n",
    "            continue\n",
    "\n",
    "        # Check against existing hashes for this species\n",
    "        is_duplicate = any(phash - existing < HASH_THRESHOLD for existing in hash_db[species_name])\n",
    "        if is_duplicate:\n",
    "            imgs_to_remove.append(img_path)\n",
    "        else:\n",
    "            hash_db[species_name].append(phash)\n",
    "            valid_images_count += 1\n",
    "\n",
    "    # Remove invalid/duplicate files\n",
    "    for img_path in imgs_to_remove:\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "            total_removed += 1\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting {img_path}: {e}\")\n",
    "\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'total_removed': total_removed,\n",
    "        'remaining_images': valid_images_count,\n",
    "        'time_seconds': time.time() - start_time,\n",
    "        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "\n",
    "    updateLogging(metadata_path, {\"cleaning\": {species_name: stats}})\n",
    "    print(f\"Finished cleaning {species_name}. Removed: {total_removed}, Remaining: {valid_images_count}\")\n",
    "    return total_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize logging\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    # Data collection structures\n",
    "    images = []\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    species_counts = defaultdict(int)\n",
    "    \n",
    "    # Process images with progress bar\n",
    "    print(\"Processing images...\")\n",
    "    for idx, specie in tqdm(enumerate(species_keys), total=len(species_keys), desc=\"Species\"):\n",
    "        specie_dir = os.path.join(DATA_DIR, specie.replace(\" \", \"_\"))\n",
    "        if not os.path.exists(specie_dir):\n",
    "            print(f\"\\nDirectory {specie_dir} does not exist. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        for img_name in os.listdir(specie_dir):\n",
    "            img_path = os.path.join(specie_dir, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                    img_array = np.array(img)\n",
    "                \n",
    "                images.append(img_array)\n",
    "                filepaths.append(img_path)\n",
    "                labels.append(idx)\n",
    "                groups.append(specie)\n",
    "                species_counts[specie] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {img_path}: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    groups = np.array(groups)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'filepath': filepaths,\n",
    "        'label_idx': y,\n",
    "        'species': groups,\n",
    "        'split': '',\n",
    "        'fold': -1\n",
    "    })\n",
    "    \n",
    "    # First split: 85% (train+val) vs 15% test\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        X,\n",
    "        test_size=CONFIG['TEST_SIZE'],\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "    df.loc[test_idx, 'split'] = 'test'\n",
    "    \n",
    "    # Cross-validation folds on train+val data (85%)\n",
    "    sgkf = StratifiedGroupKFold(\n",
    "        n_splits=CONFIG['N_SPLITS'],\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # For each fold, we'll have:\n",
    "    # - 70% train (of total)\n",
    "    # - 15% val (of total)\n",
    "    # - 15% test (fixed)\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(\n",
    "        df.loc[train_val_idx],\n",
    "        df.loc[train_val_idx, 'label_idx'],\n",
    "        df.loc[train_val_idx, 'species']\n",
    "    )):\n",
    "        # Get the original indices\n",
    "        original_train_idx = df.index[train_val_idx[train_idx]]\n",
    "        original_val_idx = df.index[train_val_idx[val_idx]]\n",
    "        \n",
    "        # Assign splits for this fold\n",
    "        df.loc[original_train_idx, 'split'] = 'train'\n",
    "        df.loc[original_train_idx, 'fold'] = fold\n",
    "        df.loc[original_val_idx, 'split'] = 'val'\n",
    "        df.loc[original_val_idx, 'fold'] = fold\n",
    "    \n",
    "    # Save CSV metadata\n",
    "    csv_path = os.path.join(DATA_DIR, f\"dataset_metadata_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSaved metadata to: {csv_path}\")\n",
    "\n",
    "    # Create HDF5 dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    h5_path = os.path.join(DATA_DIR, f\"dataset_{datetime.now().strftime('%Y%m%d')}.h5\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'w') as hf:\n",
    "        # Store all images and labels\n",
    "        hf.create_dataset('X', data=X, \n",
    "                         compression=CONFIG['COMPRESSION'],\n",
    "                         compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        hf.create_dataset('y', data=y,\n",
    "                         compression=CONFIG['COMPRESSION'],\n",
    "                         compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        \n",
    "        # Create groups for each fold\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        \n",
    "        for fold in range(CONFIG['N_SPLITS']):\n",
    "            fold_group = cv_group.create_group(f'fold_{fold}')\n",
    "            \n",
    "            # Get indices for this fold\n",
    "            train_mask = (df['fold'] == fold) & (df['split'] == 'train')\n",
    "            val_mask = (df['fold'] == fold) & (df['split'] == 'val')\n",
    "            test_mask = (df['split'] == 'test')\n",
    "            \n",
    "            # Store data for this fold\n",
    "            fold_group.create_dataset('X_train', data=X[train_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y[train_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X[val_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y[val_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_test', data=X[test_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_test', data=y[test_mask],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        \n",
    "        # Store metadata\n",
    "        hf.attrs.update({\n",
    "            'species': json.dumps(list(species_keys.keys())),\n",
    "            'image_size': json.dumps(CONFIG['IMG_SIZE']),\n",
    "            'augmentation': json.dumps(CONFIG['AUGMENTATION']),\n",
    "            'creation_time': timestamp,\n",
    "            'csv_reference': os.path.basename(csv_path),\n",
    "            'n_folds': CONFIG['N_SPLITS'],\n",
    "            'split_ratio': '70/15/15'\n",
    "        })\n",
    "    \n",
    "    # Update metadata log\n",
    "    dataset_stats = {\n",
    "        'total_images': len(df),\n",
    "        'species_counts': species_counts,\n",
    "        'split_counts': {\n",
    "            'train': len(df[df['split'] == 'train']) // CONFIG['N_SPLITS'],  \n",
    "            'val': len(df[df['split'] == 'val']) // CONFIG['N_SPLITS'],      \n",
    "            'test': len(df[df['split'] == 'test'])                           \n",
    "        },\n",
    "        'h5_path': h5_path,\n",
    "        'csv_path': csv_path,\n",
    "        'processing_time_seconds': round(time.time() - start_time, 2),\n",
    "        'timestamp': timestamp,\n",
    "        'memory_usage_mb': round(X.nbytes / (1024**2), 2)\n",
    "    }\n",
    "    \n",
    "    updateLogging(metadata_path, {\"dataset_stats\": dataset_stats})\n",
    "\n",
    "    print(f\"\\nDataset successfully created:\")\n",
    "    print(f\"- HDF5 file: {h5_path}\")\n",
    "    print(f\"- Metadata CSV: {csv_path}\")\n",
    "    print(f\"\\nSplit counts per fold:\")\n",
    "    print(f\"Train: {dataset_stats['split_counts']['train']}\")\n",
    "    print(f\"Validation: {dataset_stats['split_counts']['val']}\")\n",
    "    print(f\"Test: {dataset_stats['split_counts']['test']}\")\n",
    "    print(f\"\\nProcessing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "007f3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Carduelis+carduelis&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset creation...\n",
      "\n",
      "Downloading images for: Carduelis carduelis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:27<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 320 images from iNaturalist for Carduelis carduelis\n",
      "Downloaded image 280/280\n",
      "Downloaded 280 images from GBIF for Carduelis carduelis\n",
      "Total images downloaded for Carduelis carduelis: 600\n",
      "Starting image cleaning for: Carduelis carduelis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Carduelis carduelis: 100%|██████████| 600/600 [00:01<00:00, 535.49it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Ciconia+ciconia&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Carduelis carduelis. Removed: 1, Remaining: 599\n",
      "\n",
      "Downloading images for: Ciconia ciconia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:11<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 313 images from iNaturalist for Ciconia ciconia\n",
      "Downloaded image 287/287\n",
      "Downloaded 287 images from GBIF for Ciconia ciconia\n",
      "Total images downloaded for Ciconia ciconia: 600\n",
      "Starting image cleaning for: Ciconia ciconia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Ciconia ciconia: 100%|██████████| 600/600 [00:01<00:00, 535.53it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Columba+livia&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Ciconia ciconia. Removed: 4, Remaining: 596\n",
      "\n",
      "Downloading images for: Columba livia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [01:04<04:42,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download error for https://inaturalist-open-data.s3.amazonaws.com/photos/506132405/original.jpg: HTTPSConnectionPool(host='inaturalist-open-data.s3.amazonaws.com', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 47/200 [01:59<13:23,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download error for https://inaturalist-open-data.s3.amazonaws.com/photos/506121502/original.jpg: HTTPSConnectionPool(host='inaturalist-open-data.s3.amazonaws.com', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:16<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 324 images from iNaturalist for Columba livia\n",
      "Downloaded image 5/276\n",
      "Download error for image 2 for Columba livia: HTTPSConnectionPool(host='inaturalist-open-data.s3.amazonaws.com', port=443): Read timed out. (read timeout=10)\n",
      "Downloaded image 276/276\n",
      "Downloaded 276 images from GBIF for Columba livia\n",
      "Total images downloaded for Columba livia: 600\n",
      "Starting image cleaning for: Columba livia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Columba livia: 100%|██████████| 600/600 [00:01<00:00, 555.87it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Delichon+urbicum&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Columba livia. Removed: 0, Remaining: 600\n",
      "\n",
      "Downloading images for: Delichon urbicum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:05<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 352 images from iNaturalist for Delichon urbicum\n",
      "Downloaded image 248/248\n",
      "Downloaded 248 images from GBIF for Delichon urbicum\n",
      "Total images downloaded for Delichon urbicum: 600\n",
      "Starting image cleaning for: Delichon urbicum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Delichon urbicum: 100%|██████████| 600/600 [00:01<00:00, 582.76it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Emberiza+calandra&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Delichon urbicum. Removed: 34, Remaining: 566\n",
      "\n",
      "Downloading images for: Emberiza calandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:56<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 299 images from iNaturalist for Emberiza calandra\n",
      "Downloaded image 301/301\n",
      "Downloaded 301 images from GBIF for Emberiza calandra\n",
      "Total images downloaded for Emberiza calandra: 600\n",
      "Starting image cleaning for: Emberiza calandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Emberiza calandra: 100%|██████████| 600/600 [00:01<00:00, 511.45it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Hirundo+rustica&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Emberiza calandra. Removed: 7, Remaining: 593\n",
      "\n",
      "Downloading images for: Hirundo rustica\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:01<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 348 images from iNaturalist for Hirundo rustica\n",
      "Downloaded image 252/252\n",
      "Downloaded 252 images from GBIF for Hirundo rustica\n",
      "Total images downloaded for Hirundo rustica: 600\n",
      "Starting image cleaning for: Hirundo rustica\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Hirundo rustica: 100%|██████████| 600/600 [00:01<00:00, 576.02it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Passer+domesticus&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Hirundo rustica. Removed: 12, Remaining: 588\n",
      "\n",
      "Downloading images for: Passer domesticus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:18<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 326 images from iNaturalist for Passer domesticus\n",
      "Downloaded image 274/274\n",
      "Downloaded 274 images from GBIF for Passer domesticus\n",
      "Total images downloaded for Passer domesticus: 600\n",
      "Starting image cleaning for: Passer domesticus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Passer domesticus: 100%|██████████| 600/600 [00:01<00:00, 551.71it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Serinus+serinus&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Passer domesticus. Removed: 6, Remaining: 594\n",
      "\n",
      "Downloading images for: Serinus serinus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:20<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 282 images from iNaturalist for Serinus serinus\n",
      "Downloaded image 318/318\n",
      "Downloaded 318 images from GBIF for Serinus serinus\n",
      "Total images downloaded for Serinus serinus: 600\n",
      "Starting image cleaning for: Serinus serinus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Serinus serinus: 100%|██████████| 600/600 [00:01<00:00, 541.50it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Streptopelia+decaocto&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Serinus serinus. Removed: 2, Remaining: 598\n",
      "\n",
      "Downloading images for: Streptopelia decaocto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:35<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 357 images from iNaturalist for Streptopelia decaocto\n",
      "Downloaded image 243/243\n",
      "Downloaded 243 images from GBIF for Streptopelia decaocto\n",
      "Total images downloaded for Streptopelia decaocto: 600\n",
      "Starting image cleaning for: Streptopelia decaocto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Streptopelia decaocto: 100%|██████████| 600/600 [00:01<00:00, 548.44it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Sturnus+unicolor&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Streptopelia decaocto. Removed: 5, Remaining: 595\n",
      "\n",
      "Downloading images for: Sturnus unicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:58<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 302 images from iNaturalist for Sturnus unicolor\n",
      "Downloaded image 298/298\n",
      "Downloaded 298 images from GBIF for Sturnus unicolor\n",
      "Total images downloaded for Sturnus unicolor: 600\n",
      "Starting image cleaning for: Sturnus unicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Sturnus unicolor: 100%|██████████| 600/600 [00:01<00:00, 529.53it/s]\n",
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Turdus+merula&per_page=600&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Sturnus unicolor. Removed: 3, Remaining: 597\n",
      "\n",
      "Downloading images for: Turdus merula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:07<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 263 images from iNaturalist for Turdus merula\n",
      "Downloaded image 337/337\n",
      "Downloaded 337 images from GBIF for Turdus merula\n",
      "Total images downloaded for Turdus merula: 600\n",
      "Starting image cleaning for: Turdus merula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Turdus merula: 100%|██████████| 600/600 [00:01<00:00, 535.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning Turdus merula. Removed: 28, Remaining: 572\n",
      "Processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Species: 100%|██████████| 11/11 [00:02<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved metadata to: full_image_dataset/dataset_metadata_20250517.csv\n",
      "\n",
      "Dataset successfully created:\n",
      "- HDF5 file: full_image_dataset/dataset_20250517_213505.h5\n",
      "- Metadata CSV: full_image_dataset/dataset_metadata_20250517.csv\n",
      "\n",
      "Split counts per fold:\n",
      "Train: 907\n",
      "Validation: 197\n",
      "Test: 975\n",
      "\n",
      "Processing time: 99.67 seconds\n",
      "Tasks completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating dataset creation...\")\n",
    "metadata_path = initLogging(DATA_DIR)\n",
    "for species in species_keys.keys():\n",
    "    downloadImages(species, DATA_DIR, limit=600, metadata_path=metadata_path)\n",
    "    cleanData(species, metadata_path=metadata_path)\n",
    "createDataset(metadata_path)\n",
    "print(\"Tasks completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
