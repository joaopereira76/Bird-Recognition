{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0e34d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import requests\n",
    "import h5py\n",
    "import psutil\n",
    "import imagehash\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from pygbif import occurrences\n",
    "from pyinaturalist.node_api import get_observations\n",
    "from ebird.api import get_observations as ebird_get_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9265288",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"new_dataset\"          # Expected input dir: species_name/*.jpg\n",
    "IMG_SIZE_THRESHOLD = 200              # Min resolution (px)\n",
    "HASH_THRESHOLD = 8                    # Duplicate threshold using phash\n",
    "EBIRD_API_KEY = \"r2qmi9gi3gpg\"\n",
    "\n",
    "species_keys = {\n",
    "    \"Carduelis carduelis\": 2494686,\n",
    "    \"Ciconia ciconia\": 2481912,\n",
    "    \"Columba livia\": 2495414,\n",
    "    \"Delichon urbicum\": 2489214,\n",
    "    \"Emberiza calandra\":7634625,\n",
    "    \"Hirundo rustica\": 7192162,\n",
    "    \"Passer domesticus\": 5231190,\n",
    "    \"Serinus serinus\":2494200,\n",
    "    \"Streptopelia decaocto\": 2495696,\n",
    "    \"Sturnus unicolor\":2489104,\n",
    "    \"Turdus merula\": 6171845   \n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': (224, 224),\n",
    "    'TEST_SIZE': 0.15,\n",
    "    'TRAIN_SIZE': 0.7,\n",
    "    'VAL_SIZE': 0.1,\n",
    "    'N_SPLITS': 5,\n",
    "    'COMPRESSION': 'gzip',\n",
    "    'COMPRESSION_LEVEL': 9,\n",
    "    'AUGMENTATION': {\n",
    "        'train': [\n",
    "            {'name': 'RandomHorizontalFlip', 'p': 0.5},\n",
    "            {'name': 'RandomRotation', 'degrees': 20},\n",
    "            {'name': 'ColorJitter', 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1}\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b52404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"os\": platform.system(),\n",
    "        \"os_version\": platform.release(),\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"ram_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"python_version\": platform.python_version()\n",
    "    }\n",
    "\n",
    "def initLogging(output_dir):\n",
    "    metadata = {\n",
    "        \"config\": CONFIG,\n",
    "        \"system\": getSystemInfo(),\n",
    "        \"download\": {},\n",
    "        \"cleaning\": {},\n",
    "        \"augmentation\": {},\n",
    "        \"dataset_stats\": {},\n",
    "        \"processing_times\": {}\n",
    "    }\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata_path = os.path.join(output_dir, \"dataset_prep.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata_path\n",
    "\n",
    "def updateLogging(metadata_path, updates):\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return initLogging(os.path.dirname(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    metadata.update(updates)\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd2301b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def downloadImages(species_name, output_dir, limit=500, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    output_dir = os.path.join(DATA_DIR, species_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    print(f\"\\nDownloading images for: {species_name}\")\n",
    "    stats = {\n",
    "        'iNaturalist': 0,\n",
    "        'GBIF': 0,\n",
    "        'start_time': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        #iNaturalist download\n",
    "        stats['iNaturalist'] = downloadImages_INaturalist(species_name, output_dir, limit)\n",
    "        \n",
    "        # GBIF download\n",
    "        current_count = stats['iNaturalist']\n",
    "        stats['GBIF'] = downloadImages_GBIF(species_name, current_count, output_dir, limit - current_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "    \n",
    "    # Update metadata\n",
    "    stats.update({\n",
    "        'end_time': datetime.now().isoformat(),\n",
    "        'total_downloaded': stats['iNaturalist'] + stats['GBIF'],\n",
    "        'time_seconds': time.time() - start_time\n",
    "    })\n",
    "    \n",
    "    updateLogging(metadata_path, {\"download\": {species_name: stats}})\n",
    "    print(f\"Total images downloaded for {species_name}: {stats['total_downloaded']}\")\n",
    "    return stats['total_downloaded']\n",
    "\n",
    "def downloadImages_INaturalist(species_name, output_dir, limit=500):\n",
    "    results = get_observations(\n",
    "        taxon_name=species_name,\n",
    "        per_page=limit,\n",
    "        quality_grade=\"research\",\n",
    "        media_type=\"photo\",\n",
    "        license=[\"CC-BY\",\"CC-BY-NC\"] \n",
    "    )\n",
    "\n",
    "    images_downloaded = 0\n",
    "    seen_urls = set()\n",
    "\n",
    "    for obs in tqdm(results.get(\"results\", [])):\n",
    "        for photo in obs.get(\"photos\",[]):\n",
    "            url = photo.get(\"url\", \"\").replace(\"square\", \"original\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            try:\n",
    "                # Full-size image (not thumbnail)\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    # Apply augmentation before saving\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    img = apply_augmentation(img)  # Apply augmentation\n",
    "                    image_ext = url.split(\".\")[-1].split(\"?\")[0]\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{images_downloaded}.{image_ext}\"\n",
    "                    \n",
    "                    # Save augmented image\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            if images_downloaded >= limit:\n",
    "                break\n",
    "        if images_downloaded >= limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Downloaded {images_downloaded} images from iNaturalist for {species_name}\")\n",
    "    return images_downloaded\n",
    "\n",
    "def downloadImages_GBIF(species_name, downloadedValue, output_dir, limit=500):\n",
    "    images_downloaded = 0\n",
    "    try:\n",
    "        result = occurrences.search(\n",
    "            taxonKey=species_keys[species_name],\n",
    "            mediaType=\"StillImage\",\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        for idx, occ in enumerate(result.get(\"results\", [])):\n",
    "            for media in occ.get(\"media\",[]):\n",
    "                if media.get(\"type\") != \"StillImage\":\n",
    "                    continue\n",
    "                    \n",
    "                imgURL = media.get(\"identifier\")\n",
    "                if not imgURL:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    response = requests.get(imgURL, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    # Apply augmentation before saving\n",
    "                    img = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                    img = apply_augmentation(img)\n",
    "                    ext = 'jpg' if 'jpeg' in response.headers.get('content-type', '') else 'png'\n",
    "                    filename = f\"{species_name.replace(' ', '_')}_{downloadedValue + images_downloaded}.{ext}\"\n",
    "                    \n",
    "                    #Save\n",
    "                    img.save(os.path.join(output_dir, filename))\n",
    "                    images_downloaded += 1\n",
    "                    print(f\"Downloaded image {images_downloaded}/{limit}\", end='\\r')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError downloading image {idx} for {species_name}: {e}\")\n",
    "                    continue\n",
    "                if images_downloaded >= limit:\n",
    "                    break\n",
    "            if images_downloaded >= limit:\n",
    "                break   \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching occurrences for {species_name}: {e}\")\n",
    "    print(f\"\\nDownloaded {images_downloaded} images from GBIF for {species_name}\")\n",
    "    return images_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fc53484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return min(img.size) >= IMG_SIZE_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def getPhash(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return imagehash.phash(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def cleanData(species_name, metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "\n",
    "    hash_db = []\n",
    "    total_removed = 0\n",
    "    total_valid_images = 0\n",
    "    print(\"Starting image extraction and cleaning...\")\n",
    "\n",
    "    for species in os.listdir(DATA_DIR):\n",
    "        if species_name and species not in species_name:\n",
    "            continue\n",
    "        species_path = Path(DATA_DIR) / species\n",
    "        if not species_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        imgsRemove = []\n",
    "        valid_images = 0\n",
    "        for img_path in species_path.glob(\"*.*\"):\n",
    "            if not isValidImage(img_path):\n",
    "                imgsRemove.append(img_path)\n",
    "                continue\n",
    "\n",
    "            phash = getPhash(img_path)\n",
    "            if phash is None:\n",
    "                imgsRemove.append(img_path)\n",
    "                continue\n",
    "\n",
    "            # Check for duplicates\n",
    "            is_duplicate = any(phash - existing < HASH_THRESHOLD for existing in hash_db[species])\n",
    "            if is_duplicate:\n",
    "                imgsRemove.append(img_path)\n",
    "            else:\n",
    "                hash_db[species].append(phash)\n",
    "                valid_images += 1\n",
    "\n",
    "        # Remove invalid/duplicate files\n",
    "        for img_path in imgsRemove:\n",
    "            os.remove(img_path)\n",
    "            total_removed += 1\n",
    "\n",
    "        total_valid_images += valid_images \n",
    "\n",
    "    stats = {\n",
    "        'species': species_name,\n",
    "        'total_removed': total_removed,\n",
    "        'remaining_images': total_valid_images,\n",
    "        'time_seconds': time.time() - start_time,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    updateLogging(metadata_path, {\"cleaning\": {species_name: stats}})\n",
    "    print(f\"Finished cleaning. Total Images Removed: {total_removed}\")\n",
    "    return total_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(metadata_path=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize log if not provided\n",
    "    if metadata_path is None:\n",
    "        metadata_path = initLogging(DATA_DIR)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    species_counts = defaultdict(int)\n",
    "        \n",
    "    for idx, specie in enumerate(species_keys):\n",
    "        specie_dir = os.path.join(DATA_DIR, specie.replace(\" \", \"_\"))  \n",
    "        for img_name in os.listdir(specie_dir):\n",
    "            img_path = os.path.join(specie_dir, img_name)   \n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB').resize(CONFIG['IMG_SIZE'])\n",
    "                images.append(np.array(img))\n",
    "                labels.append(idx)\n",
    "                species_counts[specie] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['TEST_SIZE'], stratify=y\n",
    "    )\n",
    "\n",
    "    # Cross Validation with Stratified K-Folds\n",
    "    cv = StratifiedShuffleSplit(\n",
    "        n_splits=CONFIG['N_SPLITS'], \n",
    "        test_size=CONFIG['TEST_SIZE'], \n",
    "        train_size=CONFIG['TRAIN_SIZE'], \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Save to HDF5 with versioning\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"dataset_{timestamp}.h5\"\n",
    "    output_path = os.path.join(DATA_DIR, output_file)\n",
    "    \n",
    "    with h5py.File(output_path, 'w') as hf:\n",
    "        # Test set\n",
    "        test_group = hf.create_group('test')\n",
    "        test_group.create_dataset('X_test', data=X_test, \n",
    "                                compression=CONFIG['COMPRESSION'], \n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "        test_group.create_dataset('y_test', data=y_test,\n",
    "                                compression=CONFIG['COMPRESSION'],\n",
    "                                compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Cross-validation splits\n",
    "        cv_group = hf.create_group('cross_validation')\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            fold_group = cv_group.create_group(f'fold_{fold + 1}')\n",
    "            fold_group.create_dataset('X_train', data=X_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_train', data=y_train[train_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('X_val', data=X_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "            fold_group.create_dataset('y_val', data=y_train[val_idx],\n",
    "                                    compression=CONFIG['COMPRESSION'],\n",
    "                                    compression_opts=CONFIG['COMPRESSION_LEVEL'])\n",
    "\n",
    "        # Save metadata\n",
    "        hf.attrs['species'] = json.dumps(list(species_keys.keys()))\n",
    "        hf.attrs['image_size'] = json.dumps(CONFIG['IMG_SIZE'])\n",
    "        hf.attrs['augmentation'] = json.dumps(CONFIG['AUGMENTATION'])\n",
    "        hf.attrs['creation_time'] = timestamp\n",
    "    \n",
    "    # Update metadata log\n",
    "    dataset_stats = {\n",
    "        'total_images': len(images),\n",
    "        'species_counts': dict(species_counts),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'output_path': output_path,\n",
    "        'compression': CONFIG['COMPRESSION'],\n",
    "        'compression_level': CONFIG['COMPRESSION_LEVEL'],\n",
    "        'processing_time_seconds': time.time() - start_time,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    updateLogging(metadata_path, {\n",
    "        \"dataset_stats\": dataset_stats\n",
    "    })\n",
    "\n",
    "    print(f\"Dataset created at {output_path}\")\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "007f3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Carduelis+carduelis&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating dataset creation...\n",
      "\n",
      "Downloading images for: Carduelis carduelis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:31<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 292 images from iNaturalist for Carduelis carduelis\n",
      "Downloaded image 426/1708\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Ciconia+ciconia&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 427/1708\n",
      "Downloaded 427 images from GBIF for Carduelis carduelis\n",
      "Total images downloaded for Carduelis carduelis: 719\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Ciconia ciconia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:24<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 313 images from iNaturalist for Ciconia ciconia\n",
      "Downloaded image 482/1687\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Columba+livia&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 483/1687\n",
      "Downloaded 483 images from GBIF for Ciconia ciconia\n",
      "Total images downloaded for Ciconia ciconia: 796\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Columba livia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:36<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 302 images from iNaturalist for Columba livia\n",
      "Downloaded image 473/1698\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Delichon+urbicum&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 474/1698\n",
      "Downloaded 474 images from GBIF for Columba livia\n",
      "Total images downloaded for Columba livia: 776\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Delichon urbicum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:22<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 391 images from iNaturalist for Delichon urbicum\n",
      "Downloaded image 490/1609\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Emberiza+calandra&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 491/1609\n",
      "Downloaded 491 images from GBIF for Delichon urbicum\n",
      "Total images downloaded for Delichon urbicum: 882\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Emberiza calandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:58<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 325 images from iNaturalist for Emberiza calandra\n",
      "Downloaded image 523/1675\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Hirundo+rustica&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 524/1675\n",
      "Downloaded 524 images from GBIF for Emberiza calandra\n",
      "Total images downloaded for Emberiza calandra: 849\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Hirundo rustica\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:16<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 370 images from iNaturalist for Hirundo rustica\n",
      "Downloaded image 526/1630\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Passer+domesticus&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 527/1630\n",
      "Downloaded 527 images from GBIF for Hirundo rustica\n",
      "Total images downloaded for Hirundo rustica: 897\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Passer domesticus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:23<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 309 images from iNaturalist for Passer domesticus\n",
      "Downloaded image 545/1691\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Serinus+serinus&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 546/1691\n",
      "Downloaded 546 images from GBIF for Passer domesticus\n",
      "Total images downloaded for Passer domesticus: 855\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Serinus serinus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:48<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 334 images from iNaturalist for Serinus serinus\n",
      "Downloaded image 507/1666\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Streptopelia+decaocto&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 508/1666\n",
      "Downloaded 508 images from GBIF for Serinus serinus\n",
      "Total images downloaded for Serinus serinus: 842\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Streptopelia decaocto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:38<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 335 images from iNaturalist for Streptopelia decaocto\n",
      "Downloaded image 445/1665\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Sturnus+unicolor&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 446/1665\n",
      "Downloaded 446 images from GBIF for Streptopelia decaocto\n",
      "Total images downloaded for Streptopelia decaocto: 781\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Sturnus unicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:12<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 300 images from iNaturalist for Sturnus unicolor\n",
      "Downloaded image 372/1700\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Request:\n",
      "GET https://api.inaturalist.org/v1/observations?license=CC-BY%2CCC-BY-NC&quality_grade=research&taxon_name=Turdus+merula&per_page=2000&media_type=photo\n",
      "User-Agent: python-requests/2.32.3 pyinaturalist/0.20.1\n",
      "Accept-Encoding: gzip, deflate, br\n",
      "Accept: application/json\n",
      "Connection: keep-alive\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 373/1700\n",
      "Downloaded 373 images from GBIF for Sturnus unicolor\n",
      "Total images downloaded for Sturnus unicolor: 673\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "\n",
      "Downloading images for: Turdus merula\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:05<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 332 images from iNaturalist for Turdus merula\n",
      "Downloaded image 569/1668\n",
      "Downloaded 569 images from GBIF for Turdus merula\n",
      "Total images downloaded for Turdus merula: 901\n",
      "Starting image extraction and cleaning...\n",
      "Finished cleaning. Total Images Removed: 0\n",
      "Dataset created at new_dataset/dataset_20250506_024257.h5\n",
      "Total processing time: 518.53 seconds\n",
      "Tasks completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating dataset creation...\")\n",
    "metadata_path = initLogging(DATA_DIR)\n",
    "for species in species_keys.keys():\n",
    "    downloadImages(species, DATA_DIR, limit=2000, metadata_path=metadata_path)\n",
    "    cleanData(species, metadata_path=metadata_path)\n",
    "createDataset(metadata_path)\n",
    "print(\"Tasks completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
